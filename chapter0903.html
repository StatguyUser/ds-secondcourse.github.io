<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9.3 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
  </title>


  <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
  <script src="libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
 <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-13135504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-13135504-1');
</script>
<!-- End Google tag (gtag.js) -->


  <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
  <script src="javascript/cookieconsent.min.js"></script>
  <script>
    window.addEventListener("load", function () {
      window.cookieconsent.initialise({
        "palette": {
          "popup": {
            "background": "#000"
          },
          "button": {
            "background": "#f1d600"
          }
        },
        "position": "bottom-right",
        "content": {
          "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
        }
      })
    });
  </script>

  <style>
    #cta-button-desktop:hover,
    #cta-button-device:hover {
      background-color: #ffc266;
      border-color: #ffc266;
      box-shadow: none;
    }

    #cta-button-desktop,
    #cta-button-device {
      color: white;
      background-color: #ffa31a;
      text-shadow: 1px 1px 0 #444;
      text-decoration: none;
      border: 2px solid #ffa31a;
      border-radius: 10px;
      position: fixed;
      padding: 5px 10px;
      z-index: 10;
    }

    #cta-button-device {
      box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
      display: none;
      right: 20px;
      bottom: 20px;
      font-size: 20px;
    }

    #cta-button-desktop {
      box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
      display: display;
      padding: 8px 16px;
      right: 40px;
      bottom: 40px;
      font-size: 25px;
    }

    @media (max-width : 450px) {
      #cta-button-device {
        display: block;
      }

      #cta-button-desktop {
        display: none;
      }
    }
  </style>






  <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>





  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">

        <ul class="summary">
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Summary</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                class="fa fa-check"></i>Foreward</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
              href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
          <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                class="fa fa-check"></i>Before we start</a></li>
          <li class="chapter" data-level="1"><a href="introduction.html"><i class="fa fa-check"></i>Section I:
              Introduction</a>

          <li class="chapter" data-level="1"><a href="chapter01.html"><i class="fa fa-check"></i>1: Introduction</a>
          </li>

          <ul>
            <li class="chapter" data-level="2"><a href="terminology.html"><i class="fa fa-check"></i>1.1:
                Terminology</a></li>
            <li class="chapter" data-level="2"><a href="chapter0102.html"><i class="fa fa-check"></i>1.2: Process of
                Training a Machine Learning Model</a></li>
            <li class="chapter" data-level="2"><a href="chapter0103.html"><i class="fa fa-check"></i>1.3: Preventing
                Overfitting</a></li>
            <li class="chapter" data-level="2"><a href="chapter0104.html"><i class="fa fa-check"></i>1.4: Code
                Conventions</a></li>
            <li class="chapter" data-level="2"><a href="chapter0105.html"><i class="fa fa-check"></i>1.5: Datasets
                Used</a></li>
            <li class="chapter" data-level="2"><a href="chapter0106.html"><i class="fa fa-check"></i>1.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
              Feature Engineering</a>
          <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain Specific
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                Domain-Specific Feature Engineering </a></li>
            <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                References</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA Feature
              Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2: Car Sales
              </a></li>
            <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3: Coupon
                Recommendation</a></li>
            <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher Order
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                Engineering Categorical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                Engineering Ordinal Features </a></li>
            <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                Engineering Numerical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5: Interaction
              Effect Feature Engineering</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                Interaction Plot</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2: SHAP</a>
            </li>
            <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                Conclusion</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
              Feature Selection</a>
          <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6: Fundamentals of
              Feature Selection</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2: Different
                Feature Selection Methods</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3: Filter
                Method</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4: Wrapper
                Method</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
              Selection Concerning Modeling Techniques</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                Lasso, Ridge, and ElasticNet</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2: Feature
                Importance of Tree Models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3: Boruta</a>
            </li>
            <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4: Using
                Tree-Based Feature Importance for Linear Model</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5: Using
                Linear Model Feature Importance for Tree Models</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6: Linear
                Regression</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7: SVM</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8: PCA</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
              Selection Using Metaheuristic Algorithms</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1: Exhaustive
                Feature Selection</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2: Genetic
                Algorithm</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3: Simulated
                Annealing</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4: Ant Colony
                Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5: Particle
                Swarm Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
              Model Explanation</a>
          <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining Model
              and Model Predictions to Layman</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                Explainable models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                Explanation Techniques</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
              Special Chapters</a>
          <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
              Engineering & Selection for Text Classification</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2: Feature
                Construction</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3: Feature
                Selection</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4: Feature
                Extraction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5: Feature
                Reduction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                References</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things That Can
              Give Additional Improvement</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                Hyperparameter Tuning</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3: Ensemble
                Learning</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4: Signal
                Processing</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                References</a></li>
          </ul>
          </li>
        </ul>

      </nav>
    </div>
    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                <h1><span class="header-section-number">9.3:</span> Explanation
                  Techniques</h1>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Linear regression, logistic
                    regression, and decision tree discussed previously, have inherent properties
                    available to explain model behavior. Many other non-linear and complex
                    algorithms do not have any such available property to explain model behavior.
                    For any model to be of practical use, it is helpful to have some type of
                    explanation of the model. The explanation is sought for both the overall model
                    behavior and individual predictions. These explanation techniques can be
                    applied to any machine learning technique, including explainable models.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>In the following sections, 9.3.1 and
                    9.3.2, we will try to explain the models and individual predictions for the
                    hotel total rooms booking predictions dataset and hotel bookings cancellations
                    dataset. After finding the best model, the model should be trained on the whole
                    dataset, including validation, and external test data. If, however, the
                    performance of the model worsens, we should revert to the original training
                    data used within the cross-validation training sample. For this chapter, we
                    need data points to explain the model prediction. We will only keep external
                    test data and will train the model on training and validation data.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.rjefff"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.1<span
                        style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;
                      </span>Explaining Overall Model</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>There are methods available to
                    explain the model as a whole. It can explain the collective nature of the
                    model.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.3bj1y38"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.1.1 Partial
                      Dependence Plot</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>It is otherwise known as PDP. It is
                    performed at the feature level, one feature at a time. A predefined test data
                    is used for PDP. For the feature under exploration, each value is processed
                    individually. If the feature under observation is M and it has n rows, then
                    from M<sub>1</sub> to M<sub>n</sub> iteratively model predictions are obtained.
                    In each iteration, all the values in feature M are replaced with M<sub>i</sub>
                    in the test data matrix, while values for all other features are kept constant.
                    In the next step, predicted values are obtained and averaged for M<sub>i</sub>.
                    Once done, averaged predictions and different values of the feature are plotted
                    together to understand the relationship between the target and the feature.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>It can identify if the relationship
                    between the feature and the dependent variable is linear, monotonic, or more
                    complex. More the degree of change in prediction as against the change in the
                    feature, the more important the feature. It suffers from a few limitations. One
                    such limitation is that it assumes that there is no correlation between
                    different features and ignores possible feature interactions.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Let us now look at partial
                    dependence plot in figure 9.3.1.1 for one of the features in the hotel room
                    booking dataset &nbsp;<span class=SpellE>CumulativeNumberOfRoomsNet_Quartile_Encoded</span>&nbsp;.
                    We have used Lightgbm model for the plot.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="_x0000_i1059" type="#_x0000_t75" style='width:291pt;height:253.5pt;
                   visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image127.png"
                    o:title="" croptop="21607f" cropbottom="21755f" cropleft="209f" cropright="41448f"/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=388 height=338 src="images/image128.jpg" v:shapes="_x0000_i1059">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.1.1 partial dependence
                    plot of Lightgbm regression model for the hotel total room booking dataset for
                    &nbsp;<span class=SpellE>CumulativeNumberOfRoomsNet_Quartile_Encoded</span>&nbsp; feature.<o:p>
                    </o:p></span>
                </p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We can see that relationship of the
                    feature with the dependent variable is nearly linear in nature.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.1qoc8b1"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.1.2 Accumulated
                      Local Effects Plot</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>It is abbreviated as ALE plot. It
                    overcomes a major disadvantage of a partial dependence plot and can work even
                    when features are correlated.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>It studies the effect of the feature
                    at a certain level against average predictions. At a specific value for a
                    feature, it will suggest to what extent prediction is higher or lower than
                    average prediction. It takes the quantile distribution of features or specified
                    intervals within the domain of the given feature to define intervals. This
                    enables comparison among different features.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Now let us now look at the ALE plot
                    in figure 9.3.1.2 for the same scenario we discussed earlier, for the hotel
                    room booking dataset.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="_x0000_i1058" type="#_x0000_t75" style='width:283pt;height:252pt;
                   visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image129.png"
                    o:title="" croptop="21705f" cropbottom="21985f" cropleft="-280f" cropright="43551f"/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=377 height=336 src="images/image130.jpg" v:shapes="_x0000_i1058">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.1.2 accumulated local
                    effects plot of Lightgbm regression model for the hotel total room booking
                    dataset for &nbsp;<span class=SpellE>CumulativeNumberOfRoomsNet_Quartile_Encoded</span>&nbsp;
                    feature.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>As it considers quantile
                    distributions, plot is smooth at the top end. Very high values that stands out
                    in PDP is smoothened for the ACE plot. It looks more stable and easier to
                    interpret, in comparison to partial dependence plot.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.4anzqyu"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.1.3 Permutation
                      Feature Importance</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>This method uses prediction error as
                    a marker of feature importance. The List of values from the feature is selected
                    based on permutation and shuffling. The Prediction error is obtained for
                    permuted values. If prediction error increases for permuted shuffled values of
                    a feature, it is considered important.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>This is done by obtaining original
                    errors for specific test data as the first step. In the second step, for each
                    feature permutation error is obtained by selecting feature values based on
                    shuffling and permutation while keeping the value of other features constant.
                    The permutation feature importance quotient for each feature is calculated as
                    the original error divided by the permutation error of the feature. Finally,
                    the permutation feature importance quotient is sorted in descending order for
                    all features together to obtain the most and least important features in
                    descending order.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Although it takes into account all
                    types of feature interactions amongst different features, if some features are
                    correlated, it can decrease the importance of correlated features.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.2pta16n"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.1.4 Surrogate Model</span></b><b><span
                      lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Through a surrogate and
                    easy-to-explain model such as linear or logistic regression, we can try to
                    explain a black-box model. For being able to do this, 3 datasets are needed.
                    The first dataset is black-box model training data, which is used for training
                    the black-box model. The second dataset is surrogate model training data and
                    the third dataset is test data.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>After the black-box model is trained
                    using black-box model training data, features from surrogate training data are
                    used and predicted values are obtained for surrogate training data and used as
                    a dependent variable of the surrogate model. The surrogate model is trained
                    using predicted labels from the black-box model as the dependent variable and
                    features from the surrogate training dataset. Finally, both the black-box model
                    and surrogate model are used for generating predictions for test data. If the
                    performance of the black-box model and surrogate model is similar and the
                    R-square of the surrogate model is acceptable, then the surrogate model is used
                    for explaining the black-box model.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.14ykbeg"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.2<span
                        style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;
                      </span>Explaining Individual Predictions</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Understanding how the model performs
                    through the explanation of the overall model is useful. There are methods
                    available for explaining individual predictions from the model as well. These
                    techniques can be useful when we have to probe the root cause behind certain
                    predicted values from the model at an individual level. <o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.3oy7u29"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.2.1 Individual
                      Conditional Expectation Plots</span></b><b><span lang=EN style='font-family:
                  "Times New Roman",serif;mso-fareast-font-family:"Times New Roman";color:windowtext;
                  mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>It is otherwise known as the ICE
                    plot. It is related to the partial dependence plot method. However, it differs
                    in the aspect that plots are generated for individual values instead of
                    averages. One line in ICE represents one sample. It can help us understand the
                    pattern of change in prediction concerning change in a feature.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.1 has the ICE plot for
                    the &nbsp;<span class=SpellE>CumulativeNumberOfRoomsNet_Quartile_Encoded</span>&nbsp;
                    discussed earlier. We can see that the linear relationship explored between the
                    feature and dependent variable is not always true. In many cases it is
                    polynomial. As we can see the total rooms sometimes increase and then decrease
                    between different quartiles of the feature.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="_x0000_i1057" type="#_x0000_t75" style='width:247.5pt;height:225.5pt;
                   visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image131.png"
                    o:title="" croptop="21182f" cropbottom="21822f" cropleft="1051f" cropright="42781f"/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=330 height=301 src="images/image132.jpg" v:shapes="_x0000_i1057">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.1 Individual
                    Conditional Expectation plot of Lightgbm regression model for the hotel total
                    room booking dataset for the first 10 rows of external test data, for the
                    feature &nbsp;<span class=SpellE>CumulativeNumberOfRoomsNet_Quartile_Encoded</span>&nbsp;<o:p></o:p>
                    </span>
                </p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.243i4a2"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.2.2 Local
                      interpretable model-agnostic explanations</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                  color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>It is otherwise known as LIME. It
                    uses an explainable model to explain individual predictions of a black-box
                    model. To explain a specific prediction from the black-box model, a perturbed
                    sample dataset is created. To explain a specific predicted value from the
                    black-box model, all the values from the feature matrix are taken and randomly
                    changed for different features. A new dataset is created from this exercise.
                    For this dataset, prediction from the black-box model is obtained.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Perturbed samples are weighted based
                    on proximity to the original feature values which we are trying to explain. The
                    predicted value from the black-box model is used as the dependent variable and
                    weighted perturbed feature values as features for training an explainable
                    model.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Finally, the instance we were trying
                    to explain from the black-box model is explained through an explainable model.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.2 displays the LIME
                    plot of Lightgbm regression model. We have used 4th row of external test data
                    from the hotel total room booking dataset.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="_x0000_i1056" type="#_x0000_t75" style='width:519pt;height:160.5pt;
                   visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image133.png"
                    o:title="" croptop="1853f" cropleft="700f"/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=692 height=214 src="images/image134.gif" v:shapes="_x0000_i1056">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.2 LIME plot of Lightgbm
                    regression model for the hotel total room booking dataset for the 4th row of
                    external test data.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>This plot has 3 subplots. First
                    subplot has the predicted values and third subplot has actual feature names and
                    values. The second subplot has a negative and positive relationship indicator
                    against each feature. For example, for the &nbsp;<span class=SpellE><i
                        style='mso-bidi-font-style:normal'>DayofWeek_Encoded</i></span>&nbsp; feature, total
                    rooms increase in demand for days that are farther from Monday. Similarly, for
                    the &nbsp;<span class=SpellE><i
                        style='mso-bidi-font-style:normal'>AdjustedLeadTimeCumulativeNumberOfRoomsNet_Quartile_Encoded</i></span>&nbsp;
                    feature, it has a negative relationship with total room demand. This is the
                    interaction between lead time and the net number of rooms quartile feature. The
                    second part of the plot also suggests the current value for the feature,
                    against a threshold set by the model. For example, the &nbsp;<span class=SpellE><i
                        style='mso-bidi-font-style:normal'>DayOfMonth_Encoded</i></span>&nbsp; feature, has
                    a negative relationship with the total rooms sold for a check-in date. I.e.
                    Total number of rooms is sold more towards the beginning of the month, and then
                    gradually decreases as the month passes. Here the value is 20, which is higher
                    than the set threshold of 15, and the check-in date for which the model has
                    predicted is farther in the month.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.j8sehv"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.2.3 Counterfactual
                      Model Explanations</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>The counterfactual model explanation
                    is a way of explaining a model where the smallest change in a feature is
                    compared against a noticeable change in the predictable outcome. To understand
                    &quot;Noticeable outcome&quot;, let's take an example of a model which predicts
                    if someone has diabetes or not by using daily minutes of exercise, a
                    binary-coded feature for a family history of diabetes, age, and a binary-coded
                    feature for the stressful job. For someone who exercises for 45 minutes, with
                    no family history of diabetes, who is 40 years old, and who has no stress job,
                    the model prediction outcome came as non-diabetic. However, by only changing
                    jobs as stressful, the prediction came as diabetic. In this case, the
                    noticeable outcome is changing between the different classes of diabetic vs
                    non-diabetic.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Similarly, let's take a regression
                    prediction problem. We are predicting someone's income potential based on age,
                    highest qualification, and distance from the nearest metropolis. If the age is
                    below 30, the highest qualification is a bachelor's and distance is 200 miles,
                    predicted income is $40000. However, if we reduce the distance from the
                    metropolis to 10 miles, income changes to $65000. In this case, an additional
                    income of $25000 is 62.5% more than the previous salary. It can be considered a
                    &quot;noticeable outcome&quot;.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>This method tries to identify the
                    smallest change to the features which will bring noticeable outcomes. However,
                    these changed feature matrices should be similar to the original instance we
                    are trying to explain. Minimal changes should be present in additional instances
                    we are using to explain the original instance. These instances are used for
                    explaining the original instance. It will be explained in the lines &nbsp;if we make
                    &nbsp;x&nbsp; change in &nbsp;m&nbsp; feature, the outcome will change noticeably&nbsp;, as a
                    counterfactual.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.3a and 9.3.2.3b has the
                    Counterfactual plot of Lightgbm regression model for the hotel total room
                    booking dataset. We have plotted the 4th row of external test data.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="_x0000_i1055" type="#_x0000_t75" style='width:277pt;height:229pt;
                   visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image135.png"
                    o:title="" croptop="21523f" cropbottom="22050f" cropleft="1120f" cropright="43340f"/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=369 height=305 src="images/image136.jpg" v:shapes="_x0000_i1055">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.3a Counterfactual plot
                    of Lightgbm regression model for the hotel total room booking dataset for the
                    4th row of external test data, for the feature <span
                      class=SpellE>CumulativeNumberOfRoomsNet_Quartile_Encoded</span>
                    feature.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="_x0000_i1054" type="#_x0000_t75" style='width:282.5pt;height:219.5pt;
                   visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image135.png"
                    o:title="" croptop="-351f" cropbottom="43486f" cropleft="22895f" cropright="21706f"/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=377 height=293 src="images/image137.jpg" v:shapes="_x0000_i1054">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.3b Counterfactual plot
                    of Lightgbm regression model for the hotel total room booking dataset for the
                    4th row of external test data, for the feature <span
                      class=SpellE>AdjustedLeadTimeCumulativeRevenueNet_Quartile_Encoded</span>.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Previously, we have seen that the
                    feature &nbsp;<span class=SpellE>CumulativeNumberOfRoomsNet_Quartile_Encoded</span>&nbsp;
                    has the highest impact on overall model. For this instance of data, this has a
                    relatively stable and lower contribution for the output. Even if we changed the
                    values of the feature, the impact on outcome was marginal. In contrast, for the
                    feature &nbsp;<span class=SpellE>AdjustedLeadTimeCumulativeRevenueNet_Quartile_Encoded</span>&nbsp;,
                    if we slightly changed the values, the impact on output was higher than
                    previously thought.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%;background:white'><a name="_heading=h.338fx5o"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:black;mso-color-alt:windowtext'>9.3.2.4 SHAP</span></b><b><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>SHAP is the contribution of each
                    feature towards the predicted outcome from the model. To calculate the SHAP
                    value, in the first step model performance is obtained for a sample dataset. In
                    the second step, the importance of individual features is obtained by giving
                    different values to the model and observing whether model performance increases
                    or decreases. It can be positive or negative. To identify the most to least
                    impactful features, the absolute value of SHAP is considered.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We can obtain SHAP feature
                    importance for each observation in the feature matrix. This can help us
                    interpret the model globally by analyzing and summarizing the SHAP values in
                    each observation for each feature.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Now let us look at the SHAP model
                    explanation for the 4th row of external test data for Lightgbm regression in
                    figure 9.3.2.4<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p>&nbsp;</o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="_x0000_i1053" type="#_x0000_t75" style='width:6in;height:236.5pt;
                   visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image138.png"
                    o:title="" cropbottom="3874f"/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=576 height=315 src="images/image139.jpg" v:shapes="_x0000_i1053">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 9.3.2.4 SHAP plot of Lightgbm
                    regression model for the hotel total room booking dataset for the 4th row of
                    external test data.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We can see the most impactful
                    feature is &nbsp;<span class=SpellE>MonthofYear_Encoded</span>&nbsp;. This is followed by
                    &nbsp;<span class=SpellE>DayOfWeek_Encoded</span>&nbsp;. The feature &nbsp;<span
                      class=SpellE>AdjustedLeadTimeCumulativeRevenueNet_Quartile_Encoded</span>&nbsp;
                    was found to be one of the impactful features in counterfactual model explanation
                    method. SHAP found this to be the third most impactful feature for the 4<sup>th</sup>
                    sample of external test data.<o:p></o:p></span></p>
              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="chapter0902.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="chapter0904.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>


   <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "search": {
          "engine": "fuse",
          "options": null
        },
        "info": false
      });
    });
  </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>