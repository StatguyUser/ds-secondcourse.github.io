<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Chapter 11.2 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
    </title>


    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
    <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
    <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
   <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-13135504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-13135504-1');
</script>
<!-- End Google tag (gtag.js) -->


    <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
    <script src="javascript/cookieconsent.min.js"></script>
    <script>
        window.addEventListener("load", function () {
            window.cookieconsent.initialise({
                "palette": {
                    "popup": {
                        "background": "#000"
                    },
                    "button": {
                        "background": "#f1d600"
                    }
                },
                "position": "bottom-right",
                "content": {
                    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
                }
            })
        });
    </script>

    <style>
        #cta-button-desktop:hover,
        #cta-button-device:hover {
            background-color: #ffc266;
            border-color: #ffc266;
            box-shadow: none;
        }

        #cta-button-desktop,
        #cta-button-device {
            color: white;
            background-color: #ffa31a;
            text-shadow: 1px 1px 0 #444;
            text-decoration: none;
            border: 2px solid #ffa31a;
            border-radius: 10px;
            position: fixed;
            padding: 5px 10px;
            z-index: 10;
        }

        #cta-button-device {
            box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
            display: none;
            right: 20px;
            bottom: 20px;
            font-size: 20px;
        }

        #cta-button-desktop {
            box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
            display: display;
            padding: 8px 16px;
            right: 40px;
            bottom: 40px;
            font-size: 25px;
        }

        @media (max-width : 450px) {
            #cta-button-device {
                display: block;
            }

            #cta-button-desktop {
                display: none;
            }
        }
    </style>






    <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>





    <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
        <div class="book-summary">
            <nav role="navigation">

                <ul class="summary">
                    <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                                class="fa fa-check"></i>Summary</a></li>
                    <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                                class="fa fa-check"></i>Foreward</a></li>
                    <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
                            href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
                    <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                                class="fa fa-check"></i>Before we start</a></li>
                    <li class="chapter" data-level="1"><a href="section-1.html"><i class="fa fa-check"></i>Section I:
                            Introduction</a>

                    <li class="chapter" data-level="1"><a href="introduction.html"><i class="fa fa-check"></i>1:
                            Introduction</a>
                    </li>

                    <ul>
                        <li class="chapter" data-level="2"><a href="terminology.html"><i class="fa fa-check"></i>1.1:
                                Terminology</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0102.html"><i class="fa fa-check"></i>1.2:
                                Process of
                                Training a Machine Learning Model</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0103.html"><i class="fa fa-check"></i>1.3:
                                Preventing
                                Overfitting</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0104.html"><i class="fa fa-check"></i>1.4:
                                Code
                                Conventions</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0105.html"><i class="fa fa-check"></i>1.5:
                                Datasets
                                Used</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0106.html"><i class="fa fa-check"></i>1.6:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
                            Feature Engineering</a>
                    <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain
                            Specific
                            Feature Engineering </a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                                Domain-Specific Feature Engineering </a></li>
                        <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                                References</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA
                            Feature
                            Engineering </a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2:
                                Car Sales
                            </a></li>
                        <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3:
                                Coupon
                                Recommendation</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher
                            Order
                            Feature Engineering </a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                                Engineering Categorical Features</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                                Engineering Ordinal Features </a></li>
                        <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                                Engineering Numerical Features</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5:
                            Interaction
                            Effect Feature Engineering</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                                Interaction Plot</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2:
                                SHAP</a>
                        </li>
                        <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
                            Feature Selection</a>
                    <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6:
                            Fundamentals of
                            Feature Selection</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2:
                                Different
                                Feature Selection Methods</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3:
                                Filter
                                Method</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4:
                                Wrapper
                                Method</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
                            Selection Concerning Modeling Techniques</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                                Lasso, Ridge, and ElasticNet</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2:
                                Feature
                                Importance of Tree Models</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3:
                                Boruta</a>
                        </li>
                        <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4:
                                Using
                                Tree-Based Feature Importance for Linear Model</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5:
                                Using
                                Linear Model Feature Importance for Tree Models</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6:
                                Linear
                                Regression</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7:
                                SVM</a>
                        </li>
                        <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8:
                                PCA</a>
                        </li>
                        <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
                            Selection Using Metaheuristic Algorithms</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1:
                                Exhaustive
                                Feature Selection</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2:
                                Genetic
                                Algorithm</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3:
                                Simulated
                                Annealing</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4:
                                Ant Colony
                                Optimization</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5:
                                Particle
                                Swarm Optimization</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
                            Model Explanation</a>
                    <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining
                            Model
                            and Model Predictions to Layman</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                                Explainable models</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                                Explanation Techniques</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
                            Special Chapters</a>
                    <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
                            Engineering & Selection for Text Classification</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2:
                                Feature
                                Construction</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3:
                                Feature
                                Selection</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4:
                                Feature
                                Extraction</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5:
                                Feature
                                Reduction</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                                References</a></li>
                    </ul>
                    <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things
                            That Can
                            Give Additional Improvement</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                                Hyperparameter Tuning</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3:
                                Ensemble
                                Learning</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4:
                                Signal
                                Processing</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                                References</a></li>
                    </ul>
                    </li>
                </ul>

            </nav>
        </div>

        <div class="book-body">
            <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
          </h1>
        </div>

                <div class="page-wrapper" tabindex="-1" role="main">
                    <div class="page-inner">

                        <section class="normal" id="section-">
                            <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                                <h1><span class="header-section-number">11.2:</span> Hyperparameter Tuning</h1>
                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Many machine learning algorithms
                                        have hyperparameters that help the algorithm learn effectively from the data.
                                        Increasing or decreasing the value of these hyperparameters can alter the
                                        performance of the model for better or worse. For example, random forest and
                                        many other tree algorithms have a parameter for defining the number of trees
                                        that will be used for the model. By increasing or decreasing the number of
                                        trees, model performance might change.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>If the number of trees is low, say
                                        1, and we increase it to 5, the performance of the model might improve. If we
                                        further increase from 5 to 20, the model might perform even better. However, if
                                        we keep increasing the number of trees, it might not always lead to a
                                        better-performing model. After a certain number of trees, the model performance
                                        may not increase, or worse, performance might decrease. Hence it is very
                                        important to identify the number of trees in this case.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Some machine learning techniques
                                        have multiple hyperparameters. Not all hyperparameters are equally important
                                        for improving model performance. Depending on the computing power and time
                                        available, we should try hyperparameter tuning. There are many techniques
                                        available for hyperparameter tuning.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The most basic method of
                                        hyperparameter optimization is called manual search. In this method, we try all
                                        possible combinations for feature selection. A for loop can help us perform all
                                        possible combinations to fit the model with different values of hyperparameter
                                        and identify the combination at which the model gives the best performance.
                                        There is another method for hyperparameter search known as grid-search. This
                                        method is an improvised version of manual search hyperparameter optimization.
                                        The only advantage this method has is that it saves us from writing multiple
                                        nested loops.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Both manual and grid search methods
                                        are computationally expensive. Randomized Search method on the other hand
                                        trains the model on random hyperparameter combinations. It saves us from trying
                                        all combinations of hyperparameter values. One disadvantage of this method is
                                        that it may not identify the optimal values of hyperparameters.<o:p></o:p>
                                        </span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Bayes Grid Search uses Bayesian
                                        optimization to identify optimal values of hyperparameters. It can be done with
                                        the help of the python library <span class=SpellE><span style='color:white;
background:#333333'>skopt</span></span>. There is another python library <span class=SpellE><span
                                                style='color:white;background:#333333'>optuna</span></span>,
                                        that uses the define-by-run principle to enable the user dynamically construct
                                        the search space. It allows the user ways that have never been possible with
                                        previous hyperparameter tuning frameworks. It does so by combining efficient
                                        searching and pruning algorithms, and thereby greatly improves the
                                        cost-effectiveness of optimization.<o:p></o:p></span></p>
                            </div>
                        </section>

                    </div>
                </div>
            </div>
            <a href="chapter11.html" class="navigation navigation-prev " aria-label="Previous page"><i
                    class="fa fa-angle-left"></i></a>
            <a href="chapter1103.html" class="navigation navigation-next " aria-label="Next page"><i
                    class="fa fa-angle-right"></i></a>
        </div>
    </div>
    <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
    <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
    <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>


     <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "search": {
          "engine": "fuse",
          "options": null
        },
        "info": false
      });
    });
  </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
    <script>
        (function () {
            var script = document.createElement("script");
            script.type = "text/javascript";
            var src = "true";
            if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
            if (location.protocol !== "file:")
                if (/^https?:/.test(src))
                    src = src.replace(/^https?:/, '');
            script.src = src;
            document.getElementsByTagName("head")[0].appendChild(script);
        })();
    </script>
</body>

</html>