<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Chapter 1.3 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
    </title>


    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
    <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
    <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
    <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
   <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-13135504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-13135504-1');
</script>
<!-- End Google tag (gtag.js) -->


    <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
    <script src="javascript/cookieconsent.min.js"></script>
    <script>
        window.addEventListener("load", function () {
            window.cookieconsent.initialise({
                "palette": {
                    "popup": {
                        "background": "#000"
                    },
                    "button": {
                        "background": "#f1d600"
                    }
                },
                "position": "bottom-right",
                "content": {
                    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
                }
            })
        });
    </script>

    <style>
        #cta-button-desktop:hover,
        #cta-button-device:hover {
            background-color: #ffc266;
            border-color: #ffc266;
            box-shadow: none;
        }

        #cta-button-desktop,
        #cta-button-device {
            color: white;
            background-color: #ffa31a;
            text-shadow: 1px 1px 0 #444;
            text-decoration: none;
            border: 2px solid #ffa31a;
            border-radius: 10px;
            position: fixed;
            padding: 5px 10px;
            z-index: 10;
        }

        #cta-button-device {
            box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
            display: none;
            right: 20px;
            bottom: 20px;
            font-size: 20px;
        }

        #cta-button-desktop {
            box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
            display: display;
            padding: 8px 16px;
            right: 40px;
            bottom: 40px;
            font-size: 25px;
        }

        @media (max-width : 450px) {
            #cta-button-device {
                display: block;
            }

            #cta-button-desktop {
                display: none;
            }
        }
    </style>






    <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>





    <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
        <div class="book-summary">
            <nav role="navigation">

                <ul class="summary">
                    <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                                class="fa fa-check"></i>Summary</a></li>
                    <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                                class="fa fa-check"></i>Foreward</a></li>
                    <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
                            href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
                    <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                                class="fa fa-check"></i>Before we start</a></li>
                    <li class="chapter" data-level="1"><a href="section-1.html"><i class="fa fa-check"></i>Section I:
                            Introduction</a>

                    <li class="chapter" data-level="1"><a href="introduction.html"><i class="fa fa-check"></i>1:
                            Introduction</a>
                    </li>

                    <ul>
                        <li class="chapter" data-level="2"><a href="terminology.html"><i class="fa fa-check"></i>1.1:
                                Terminology</a></li>
                        <li class="chapter" data-level="2"><a href="training-a-model.html"><i class="fa fa-check"></i>1.2:
                                Process of
                                Training a Machine Learning Model</a></li>
                        <li class="chapter" data-level="2"><a href="preventing-overfitting.html"><i class="fa fa-check"></i>1.3:
                                Preventing
                                Overfitting</a></li>
                        <li class="chapter" data-level="2"><a href="code-conventions.html"><i class="fa fa-check"></i>1.4:
                                Code
                                Conventions</a></li>
                        <li class="chapter" data-level="2"><a href="datasets-used.html"><i class="fa fa-check"></i>1.5:
                                Datasets
                                Used</a></li>
                        <li class="chapter" data-level="2"><a href="chapter1-references.html"><i class="fa fa-check"></i>1.6:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
                            Feature Engineering</a>
                    <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain
                            Specific
                            Feature Engineering </a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                                Domain-Specific Feature Engineering </a></li>
                        <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                                References</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA
                            Feature
                            Engineering </a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2:
                                Car Sales
                            </a></li>
                        <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3:
                                Coupon
                                Recommendation</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher
                            Order
                            Feature Engineering </a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                                Engineering Categorical Features</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                                Engineering Ordinal Features </a></li>
                        <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                                Engineering Numerical Features</a></li>
                        <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5:
                            Interaction
                            Effect Feature Engineering</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                                Interaction Plot</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2:
                                SHAP</a>
                        </li>
                        <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
                            Feature Selection</a>
                    <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6:
                            Fundamentals of
                            Feature Selection</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2:
                                Different
                                Feature Selection Methods</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3:
                                Filter
                                Method</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4:
                                Wrapper
                                Method</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
                            Selection Concerning Modeling Techniques</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                                Lasso, Ridge, and ElasticNet</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2:
                                Feature
                                Importance of Tree Models</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3:
                                Boruta</a>
                        </li>
                        <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4:
                                Using
                                Tree-Based Feature Importance for Linear Model</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5:
                                Using
                                Linear Model Feature Importance for Tree Models</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6:
                                Linear
                                Regression</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7:
                                SVM</a>
                        </li>
                        <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8:
                                PCA</a>
                        </li>
                        <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                                Conclusion</a></li>
                    </ul>
                    <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
                            Selection Using Metaheuristic Algorithms</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1:
                                Exhaustive
                                Feature Selection</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2:
                                Genetic
                                Algorithm</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3:
                                Simulated
                                Annealing</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4:
                                Ant Colony
                                Optimization</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5:
                                Particle
                                Swarm Optimization</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
                            Model Explanation</a>
                    <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining
                            Model
                            and Model Predictions to Layman</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                                Explainable models</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                                Explanation Techniques</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4:
                                Putting
                                Everything Together</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                                References</a></li>
                    </ul>
                    </li>
                    <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
                            Special Chapters</a>
                    <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
                            Engineering & Selection for Text Classification</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2:
                                Feature
                                Construction</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3:
                                Feature
                                Selection</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4:
                                Feature
                                Extraction</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5:
                                Feature
                                Reduction</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                                References</a></li>
                    </ul>
                    <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things
                            That Can
                            Give Additional Improvement</a>
                    </li>
                    <ul>
                        <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                                Introduction</a></li>
                        <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                                Hyperparameter Tuning</a></li>
                        <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3:
                                Ensemble
                                Learning</a></li>
                        <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4:
                                Signal
                                Processing</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                                Conclusion</a></li>
                        <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                                References</a></li>
                    </ul>
                    </li>
                </ul>

            </nav>
        </div>
        <div class="book-body">
            <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
          </h1>
        </div>


                <div class="page-wrapper" tabindex="-1" role="main">
                    <div class="page-inner">

                        <section class="normal" id="section-">
                            <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                                <h1 style="font-size: 1.78em;"><span class="header-section-number">1.3:</span>
                                    Preventing
                                    Overfitting
                                </h1>
                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>A dataset can be divided into a
                                        training dataset, and external test data in an 80:20, 70:30, or even 60:40
                                        ratio, depending on the availability of data. Then a further split for
                                        validation data is created from the remaining training data. The remaining
                                        training dataset can be divided further into train and development test data in
                                        the same ratio. We then train our model in iterations after performing feature
                                        engineering and feature selection. At each iteration, we train our model, and
                                        we test its performance on the development test data and validation data. Once
                                        we get a model of acceptable performance, we test it on external test data to
                                        ensure it generalizes well on unseen data.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Bias-variance tradeoff tells us that
                                        by adding additional features, overfitting can happen. Overfitting can also
                                        happen by doing feature selection. If the model has learned our data in a way
                                        that
                                        gives the best predictive power for the dependent variable, it doesn't
                                        guarantee that it will generalize well on unseen data. What it only guarantees
                                        is that it can perform well on the development test data and validation data
                                        for validating our model.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>One way to overcome this challenge
                                        is to test your model on multiple development test data to understand how well
                                        it can generalize. This can be performed by cross-validation.<o:p></o:p></span>
                                </p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Generally, we perform 5-fold
                                        cross-validation. We obtain the training dataset after separating external test
                                        data and validation data. The remaining training data is divided into 5 equal
                                        parts, and 4 parts are used for training. The remaining 1 part is for the
                                        development test. We also use validation data for measuring model performance.
                                        We repeat this 5 times and average the model metric across 5 samples for the
                                        development test data and validation data. Doing this reduces the likelihood of
                                        overfitting. In addition to cross-validation, we will test the model on
                                        external test data to ensure that indeed the cross-validation score is
                                        reliable.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>If the average of the model metric
                                        of 5 cross-validations is very different from the model metric of external test
                                        data, we will need to do further probing in the dataset to check if the data
                                        distribution is very different in 5-fold cross-validation and validation data
                                        Vs external test data. The object of the probe should be to ensure that data
                                        distribution is close to the real-world values in all 3 datasets. i.e. 1)
                                        training, 2) development test data in all cross-validation samples, 3)
                                        validation data, as well as in 4) external test data. If any anomaly is
                                        identified, it should be subject to further investigation.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Using 5-fold cross-validation is
                                        subject to the availability of data. Although used as a universal standard, we
                                        should use 5-fold only when we have sufficient data. In the absence of volume,
                                        we can increase cross-validation from 5-fold to 10-fold, to get a more robust
                                        averaged model metric. 'More' and 'less' are subjective when it comes to
                                        datasets.
                                        For example, for the training of the Twitter sentiment classifier, 8000 tweets
                                        can be considered small, if we compare it against billions of tweets present in
                                        the Twitter database. If we still have to use such a small number of tweets
                                        dataset for modeling, it will be wise to use 10-fold cross-validation. However,
                                        while modeling the average lifespan of endangered species, 8000 observations
                                        can be considered voluminous data and we can limit cross-validation to 5-fold.
                                        <o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>If the data has distinct strata, we
                                        should ensure that train and test samples in cross-validation, as well as
                                        external hold-out samples, should all be representative of all the strata. We
                                        should proceed with stratified k-fold cross-validation in such cases. For
                                        example, if we are developing a model for predicting the likelihood of
                                        occurrence of a specific disease amongst people living in the US, given all the
                                        body vitals and characteristics such as blood pressure, sugar level, heartbeat
                                        rate, sodium level, cholesterol, daily minutes of exercise, body weight,
                                        height, etc. Such a dataset should be collected from all the states. Overall
                                        health and well-being might have less impact on a person based on which state
                                        they belong to. Hence, we cannot use the state as a feature to train our model.
                                        However, we should ensure that in the training and test samples across all
                                        cross-validations, each state should have an equal percentage of observations.
                                        This will ensure that model will generalize well to real-world circumstances.
                                        <o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>While doing train vs test split and
                                        even for external test data, we should give due regard to the unit of data.
                                        Most often one single observation is a unit of data. We can split such a
                                        dataset into cross-validation samples. However, when a data observation belongs
                                        to a distinguishable group of similar observations, as a single unit, we will
                                        consider it as a unit of data.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>For example, if we are evaluating
                                        the effectiveness of new medicine in regulating the blood pressure of
                                        individual patients, we will record the dosage amount and resultant blood
                                        pressure for multiple days. In this case, data recorded for all days for a
                                        specific patient will be a unit of data. While splitting data into the training
                                        set, external test data, and development test data, we should ensure that a
                                        patient's data is present entirely either in training, external test data,
                                        validation data, or development test data. In no case, a patient's data should
                                        be present in more than one group from training, external test data, or
                                        development test data. If a patient's data is present in both training and test
                                        data simultaneously, then the results could be biased, as it will lead to
                                        over-optimistic results for the same patient, as the model is already familiar
                                        with similar values for the patient in training data. At an aggregate level, it
                                        can give us overoptimistic model performance. Hence, we should identify units
                                        of data and should ensure that data from a single unit is not present in
                                        training, test, and/or external test data simultaneously.<o:p></o:p></span></p>

                                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>If cross-validation is done right,
                                        feature engineering and feature selection can help us identify a smaller set of
                                        features that give the best performance for predicting the target variable.<o:p>
                                        </o:p></span></p>

                            </div>
                        </section>

                    </div>
                </div>
            </div>
            <a href="training-a-model.html" class="navigation navigation-prev " aria-label="Previous page"><i
                    class="fa fa-angle-left"></i></a>
            <a href="code-conventions.html" class="navigation navigation-next " aria-label="Next page"><i
                    class="fa fa-angle-right"></i></a>
        </div>
    </div>
    <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
    <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
    <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
    <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>


     <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "search": {
          "engine": "fuse",
          "options": null
        },
        "info": false
      });
    });
  </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
    <script>
        (function () {
            var script = document.createElement("script");
            script.type = "text/javascript";
            var src = "true";
            if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
            if (location.protocol !== "file:")
                if (/^https?:/.test(src))
                    src = src.replace(/^https?:/, '');
            script.src = src;
            document.getElementsByTagName("head")[0].appendChild(script);
        })();
    </script>
</body>

</html>