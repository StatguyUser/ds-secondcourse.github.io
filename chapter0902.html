<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9.2 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
  </title>


  <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
  <script src="libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
 <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-13135504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-13135504-1');
</script>
<!-- End Google tag (gtag.js) -->


  <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
  <script src="javascript/cookieconsent.min.js"></script>
  <script>
    window.addEventListener("load", function () {
      window.cookieconsent.initialise({
        "palette": {
          "popup": {
            "background": "#000"
          },
          "button": {
            "background": "#f1d600"
          }
        },
        "position": "bottom-right",
        "content": {
          "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
        }
      })
    });
  </script>

  <style>
    #cta-button-desktop:hover,
    #cta-button-device:hover {
      background-color: #ffc266;
      border-color: #ffc266;
      box-shadow: none;
    }

    #cta-button-desktop,
    #cta-button-device {
      color: white;
      background-color: #ffa31a;
      text-shadow: 1px 1px 0 #444;
      text-decoration: none;
      border: 2px solid #ffa31a;
      border-radius: 10px;
      position: fixed;
      padding: 5px 10px;
      z-index: 10;
    }

    #cta-button-device {
      box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
      display: none;
      right: 20px;
      bottom: 20px;
      font-size: 20px;
    }

    #cta-button-desktop {
      box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
      display: display;
      padding: 8px 16px;
      right: 40px;
      bottom: 40px;
      font-size: 25px;
    }

    @media (max-width : 450px) {
      #cta-button-device {
        display: block;
      }

      #cta-button-desktop {
        display: none;
      }
    }
  </style>






  <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>





  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">

        <ul class="summary">
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Summary</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                class="fa fa-check"></i>Foreward</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
              href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
          <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                class="fa fa-check"></i>Before we start</a></li>
          <li class="chapter" data-level="1"><a href="section-1.html"><i class="fa fa-check"></i>Section I:
              Introduction</a>

          <li class="chapter" data-level="1"><a href="introduction.html"><i class="fa fa-check"></i>1: Introduction</a>
          </li>

          <ul>
            <li class="chapter" data-level="2"><a href="terminology.html"><i class="fa fa-check"></i>1.1:
                Terminology</a></li>
            <li class="chapter" data-level="2"><a href="training-a-model.html"><i class="fa fa-check"></i>1.2: Process of
                Training a Machine Learning Model</a></li>
            <li class="chapter" data-level="2"><a href="preventing-overfitting.html"><i class="fa fa-check"></i>1.3: Preventing
                Overfitting</a></li>
            <li class="chapter" data-level="2"><a href="code-conventions.html"><i class="fa fa-check"></i>1.4: Code
                Conventions</a></li>
            <li class="chapter" data-level="2"><a href="datasets-used.html"><i class="fa fa-check"></i>1.5: Datasets
                Used</a></li>
            <li class="chapter" data-level="2"><a href="chapter1-references.html"><i class="fa fa-check"></i>1.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
              Feature Engineering</a>
          <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain Specific
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                Domain-Specific Feature Engineering </a></li>
            <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                References</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA Feature
              Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2: Car Sales
              </a></li>
            <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3: Coupon
                Recommendation</a></li>
            <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher Order
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                Engineering Categorical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                Engineering Ordinal Features </a></li>
            <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                Engineering Numerical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5: Interaction
              Effect Feature Engineering</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                Interaction Plot</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2: SHAP</a>
            </li>
            <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                Conclusion</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
              Feature Selection</a>
          <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6: Fundamentals of
              Feature Selection</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2: Different
                Feature Selection Methods</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3: Filter
                Method</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4: Wrapper
                Method</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
              Selection Concerning Modeling Techniques</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                Lasso, Ridge, and ElasticNet</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2: Feature
                Importance of Tree Models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3: Boruta</a>
            </li>
            <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4: Using
                Tree-Based Feature Importance for Linear Model</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5: Using
                Linear Model Feature Importance for Tree Models</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6: Linear
                Regression</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7: SVM</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8: PCA</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
              Selection Using Metaheuristic Algorithms</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1: Exhaustive
                Feature Selection</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2: Genetic
                Algorithm</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3: Simulated
                Annealing</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4: Ant Colony
                Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5: Particle
                Swarm Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
              Model Explanation</a>
          <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining Model
              and Model Predictions to Layman</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                Explainable models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                Explanation Techniques</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
              Special Chapters</a>
          <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
              Engineering & Selection for Text Classification</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2: Feature
                Construction</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3: Feature
                Selection</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4: Feature
                Extraction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5: Feature
                Reduction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                References</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="baseline-removal-python.html"><i class="fa fa-check"></i>11: Things That Can
              Give Additional Improvement</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="baseline-removal-python.html"><i class="fa fa-check"></i>11.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i>11.2:
                Hyperparameter Tuning</a></li>
            <li class="chapter" data-level="2.3"><a href="ensemble-learning.html"><i class="fa fa-check"></i>11.3: Ensemble
                Learning</a></li>
            <li class="chapter" data-level="2.4"><a href="signal-processing.html"><i class="fa fa-check"></i>11.4: Signal
                Processing</a></li>
            <li class="chapter" data-level="2.5"><a href="conclusion.html"><i class="fa fa-check"></i>11.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="references.html"><i class="fa fa-check"></i>11.6:
                References</a></li>
          </ul>
          </li>
        </ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                <h1><span class="header-section-number">9.2:</span> Explainable models</h1>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Explainable models have inherent
                    attributes that explain the impact that each feature has as weights or by
                    creating a tree structure to explain the hierarchy of relationships amongst different
                    features. The most prominent explainable models are linear regression, logistic
                    regression, and decision trees.<o:p></o:p></span></p>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Explainable models have inherent
                    attributes that explain the impact that each feature has as weights or by
                    creating a tree structure to explain the hierarchy of relationships amongst different
                    features. The most prominent explainable models are linear regression, logistic
                    regression, and decision trees.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.2y3w247"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>9.2.1<span style='mso-tab-count:1'> </span>Linear
                      Regression</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The linear regression models can
                    explain at an overall level, the importance of each feature through the beta coefficient.<o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Its functional form is Y =
                  </span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif'><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:7pt;height:15pt'>
 <v:imagedata src="images/image113.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=9 height=20 src="images/image114.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>0
                    </span></sub><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>+
                  </span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif'><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:7pt;height:15pt'>
 <v:imagedata src="images/image113.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=9 height=20 src="images/image114.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>1</span></sub><span
                    lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman"'>x<sub>1</sub> + .. +</span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif;
 mso-fareast-font-family:"Cambria Math";mso-bidi-font-family:"Cambria Math"'><m:r>
  </m:r><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:9.5pt;height:15pt'>
 <v:imagedata src="images/image115.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=13 height=20 src="images/image116.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>n</span></sub><span
                    class=SpellE><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>x<sub>n</sub></span></span><sub><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman"'>
                      <o:p></o:p>
                    </span></sub>
                </p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Y is the predicted value of the
                    dependent or outcome variable.
                  </span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif'><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:7pt;height:15pt'>
 <v:imagedata src="images/image113.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=9 height=20 src="images/image114.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>0</span></sub><span
                    lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman"'> is a constant term,
                  </span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif'><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:7pt;height:15pt'>
 <v:imagedata src="images/image113.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=9 height=20 src="images/image114.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>1
                    </span></sub><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>is the weight of the first feature
                    x1 and</span><!--[if gte msEquation 12]><m:oMath><i style='mso-bidi-font-style:
 normal'><span lang=EN style='font-family:"Cambria Math",serif;mso-fareast-font-family:
 "Cambria Math";mso-bidi-font-family:"Cambria Math"'><m:r> </m:r><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:9.5pt;height:15pt'>
 <v:imagedata src="images/image115.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=13 height=20 src="images/image116.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>n
                    </span></sub><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>is the weight of the nth feature <span class=SpellE>x<sub>n</sub></span>. If
                    features are standardized and on the same
                    scale, we can make comparisons amongst features to identify the most and least
                    impactful features. In addition to this, through adjusted R<sup>2</sup>, we can
                    ascertain to what extent the model is better than a simple horizontal line
                    through the mean value of the dependent variable.<o:p></o:p></span>
                </p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>By changing the value in each
                    feature, we can observe the impact on the prediction outcome. Predictions are
                    the sum product of weights and feature values. For numerical features, values
                    can be increased or decreased. Categorical features can be represented as
                    binary encoded 1 or 0 dummy features and the effect of their presence or
                    absence can be compared with the model outcome. Modeling fashion clothing involvement
                    <sup>[1]</sup> as the outcome, we can make inferences from table 9.2.1.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image43.png" o:spid="_x0000_i1062" type="#_x0000_t75" style='width:468pt;
 height:198pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image117.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=624 height=264 src="images/image118.jpg" v:shapes="image43.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Table 9.2.1 Regression output for
                    fashion clothing involvement<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Although the model didn't explain
                    all the variance in the data as evident from a low R square, it did however
                    indicate that normative behavior is the biggest predictor of fashion clothing
                    involvement. Normative influences are defined as the degree to which people
                    conform to the expectations of society. Apart from this, age is negatively
                    associated with fashion clothing involvement. Young people tend to be more
                    involved in fashion clothing than those who are old. To a certain extent,
                    married people are more fashionable than those who are unmarried.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.1d96cc0"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>9.2.2<span style='mso-tab-count:1'> </span>Logistic
                      Regression</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The logistic regression follows the
                    sigmoid function, which takes values between 0 and 1. 1 is the desired outcome.
                    In total, the odds of events happening and not happening is 1. The cut-off is
                    0.5, this is otherwise known as the decision boundary.</span><!--[if gte vml 1]><v:shape
 id="image26.png" o:spid="_x0000_s2050" type="#_x0000_t75" style='position:absolute;
 margin-left:103.5pt;margin-top:51pt;width:36.35pt;height:26.4pt;z-index:251658240;
 visibility:visible;mso-wrap-style:square;mso-wrap-distance-left:9pt;
 mso-wrap-distance-top:9pt;mso-wrap-distance-right:9pt;
 mso-wrap-distance-bottom:9pt;mso-position-horizontal:absolute;
 mso-position-horizontal-relative:text;mso-position-vertical:absolute;
 mso-position-vertical-relative:text'>
 <v:imagedata src="images/image119.png"
  o:title="" croptop="40731f" cropleft="29101f" cropright="24989f"/>
</v:shape><![endif]-->
                  <![if !vml]><span style='mso-ignore:vglayout;position:
relative;z-index:251658240'><span style='position:absolute;left:222px;
top:-77955px;width:73px;height:1px'><img width=49 height=1 src="images/image120.jpg"
                        v:shapes="image26.png"></span></span>
                  <![endif]><span lang=EN style='font-family:
"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span>
                </p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Its functional form is<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Where
                  </span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif;
 mso-fareast-font-family:"Cambria Math";mso-bidi-font-family:"Cambria Math"'><m:r>z</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:5.5pt;height:15pt'>
 <v:imagedata src="images/image121.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=7 height=20 src="images/image122.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><span lang=EN
                    style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'><span
                      style='mso-spacerun:yes'> </span>is a linear regression function and is equal
                    to </span><!--[if gte msEquation 12]><m:oMath><i style='mso-bidi-font-style:
 normal'><span lang=EN style='font-family:"Cambria Math",serif'><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:7pt;height:15pt'>
 <v:imagedata src="images/image113.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=9 height=20 src="images/image114.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>0
                    </span></sub><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>+
                  </span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif'><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:7pt;height:15pt'>
 <v:imagedata src="images/image113.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=9 height=20 src="images/image114.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>1</span></sub><span
                    lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman"'>x<sub>1</sub> + .. +</span><!--[if gte msEquation 12]><m:oMath><i
 style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Cambria Math",serif;
 mso-fareast-font-family:"Cambria Math";mso-bidi-font-family:"Cambria Math"'><m:r>
  </m:r><m:r>&#946;</m:r></span></i></m:oMath><![endif]-->
                  <![if !msEquation]><span lang=EN style='font-size:11.0pt;line-height:115%;font-family:"Arial",sans-serif;
mso-fareast-font-family:Arial;position:relative;top:4.5pt;mso-text-raise:-4.5pt;
mso-ansi-language:EN;mso-fareast-language:EN-IN;mso-bidi-language:AR-SA'><!--[if gte vml 1]><v:shape
 id="_x0000_i1025" type="#_x0000_t75" style='width:9.5pt;height:15pt'>
 <v:imagedata src="images/image115.png"
  o:title="" chromakey="white"/>
</v:shape><![endif]-->
                    <![if !vml]><img width=13 height=20 src="images/image116.gif" v:shapes="_x0000_i1025">
                    <![endif]>
                  </span>
                  <![endif]><sub><span lang=EN
                      style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman"'>n</span></sub><span
                    class=SpellE><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>x<sub>n</sub></span></span><sub><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman"'>
                      <o:p></o:p>
                    </span></sub>
                </p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Predictions are probability values.
                    For numerical features, values can be increased or decreased. Categorical
                    features can be represented as binary encoded 1 or 0 dummy features. The effect
                    of change in numerical and categorical dummy binary features on the outcome
                    variable can be observed through the change in the position of the predicted
                    outcome for the decision boundary.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>For example, if for the baseline
                    case the predicted outcome is 0.4 and after changing the value in features
                    predicted outcome is 0.55, we can infer that the predicted class has changed
                    from 0 to 1. For the value 0.4, as per the decision boundary of 0.5, it will be
                    reduced to 0 as the outcome and for the predicted value of 0.55, it will be
                    changed to 1.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The coefficients returned by
                    logistic regression for each feature are the log of odds. Mathematically, it
                    can be represented as a <i style='mso-bidi-font-style:normal'>log (probability
                      of event / (1-probability of the event))</i>. Before interpreting the
                    coefficients of the logistic regression beta coefficient, we need to convert
                    the log of odds into the interpretable format. This can be done by first
                    converting the log into the exponential form using the formula odd =
                    exponential (original form log of odd). Finally, by doing odds/(1 + odds), we can
                    compare the coefficients of each feature if they are all standardized.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>We can analyze the psychological
                    impact of COVID-19 among primary healthcare workers through logistic regression
                    from the below logistic regression output <sup>[2]</sup>. Table 9.2.2 has
                    output from the logistic regression model.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image27.png" o:spid="_x0000_i1061" type="#_x0000_t75" style='width:412.5pt;
 height:253.5pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image123.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=550 height=338 src="images/image124.jpg" v:shapes="image27.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Table 9.2.2 Regression output for
                    psychological impact of COVID-19 among primary healthcare workers<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Older healthcare workers &quot;55
                    years or older&quot; were four times more vulnerable to depression than younger
                    workers. Healthcare workers who were neither worried for themselves nor for
                    their families were found to be less likely to have depression disorder in
                    comparison to those who worried for themselves and their families.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>In the case of text classification,
                    we can obtain logistic regression weights for each corresponding word feature.
                    This will help us explain the relative importance of each word.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.3x8tuzt"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>9.2.3<span style='mso-tab-count:1'> </span>Decision
                      Tree</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The decision trees can be trained to
                    identify different classes for a classification problem. It can also be trained
                    for a regression problem. Training the decision tree is otherwise called
                    growing a decision tree. It is performed through a splitting process. Adding a
                    section to a tree is called grafting whereas cutting a tree or its node is
                    called pruning. The dependent variable is split with the help of independent
                    variables at nodes with the most optimal value into branches based on the best
                    split. The best split is decided based on either of the methods such as the
                    Gini index, information gain, or chi-square.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>CART (Classification and Regression
                    Trees) uses Gini as the method to evaluate split into data. For a pure
                    population, the score is 0. This is most useful in noisy data. If the dependent
                    variable has <b style='mso-bidi-font-weight:normal'><i style='mso-bidi-font-style:
normal'>n</i></b> number of classes, for each feature, the Gini impurity is
                    calculated and the one with the lowest impurity is chosen. In the case of
                    information gain or entropy, after calculating the information gain for each
                    feature, the one with the highest is selected for the node splitting.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>For numerical features, exclusion
                    points are selected and values less than and higher than the exclusion point
                    are encoded as ordinal levels. This makes the feature binary. Through n
                    possible exclusion points, the one which gives maximum information gain or
                    minimum Gini impurity is selected.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>A study for potential diabetes
                    mellitus created a decision tree to model potential risk for diabetes based on
                    diet, lifestyle, and past family history of diabetes <sup>[3]</sup>. Figure
                    9.2.3 has the decision tree diagram.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image61.png" o:spid="_x0000_i1060" type="#_x0000_t75" style='width:468pt;
 height:255pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image125.png"
  o:title="" croptop="886f" cropleft="671f"/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=624 height=340 src="images/image126.jpg" v:shapes="image61.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Figure 9.2.3 Potential risk for diabetes
                    using decision tree<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>If we traverse from top to bottom of
                    the decision tree across 'nodes', data is automatically filtered as a subset
                    based on the 'and' condition. For example, let's traverse the right edge of the
                    tree as Age(&gt;40) -&gt; Work Stress(High) -&gt; Family History(Yes) -&gt;
                    Diabetes(85). The node and edges can be translated as people with age above 40
                    and who also have high work stress and family history of other family members
                    having been diagnosed with diabetes, it is certain that the person will have
                    diabetes. All the 85 observations falling under this group are diagnosed with
                    diabetes.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>We can scale the Gini index or
                    entropy at each node so that it adds to 100. It can help us compare between
                    different subsets of data created with decision tree &quot;AND&quot; rules. We
                    can narrow down to identify the most and least impactful subsets of data and
                    rules affecting the dependent variable.<o:p></o:p></span></p>
              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="chapter09.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="chapter0903.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>


   <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "search": {
          "engine": "fuse",
          "options": null
        },
        "info": false
      });
    });
  </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>