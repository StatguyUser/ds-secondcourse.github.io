[
 [
 "index.html",
 "Feature Engineering & Selection for Explainable Models",
 " Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists Md Azimul Haque   You can buy the full version of the book from Amazon. This web version of the book has content only for chapter-1 & chapter-11.   You can cite my book as: Feature Engineering & Selection for Explainable Models: A Second Course for Data Scientists (Revised Edition), Md Azimul Haque, 2024   Connect with author: Twitter, Linkedin   While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.   © Copyright 2022 Md Azimul Haque. All Rights Reserved. "
 ],
 [
 "foreward.html",
 "Foreward",
 " Foreward Over the years, I've had the privilege of standing before countless students as a part-time faculty member in data science. Guiding hundreds of aspiring minds, I've witnessed many of them successfully transition into rewarding careers in data science and machine learning. It's been more than just teaching; it's been about igniting a passion and watching it transform lives. The journey hasn't stopped at the classroom door. Often, former students reach out as they embark on their first real-world machine learning projects, seeking advice and guidance. These interactions unveiled a pattern of common challenges that new data scientists face. I noticed that many struggled with practical aspects of feature engineering, feature selection, and signal processing—areas crucial for building effective machine learning models but lacking in accessible Python implementations. This realization motivated me to contribute to the open-source community by developing Python libraries that fill these gaps. Over the years, I've coded and shared numerous methods that weren't previously available, aiming to empower researchers and practitioners alike.The response has been overwhelming. Researchers worldwide have approached me, eager to understand how to leverage these algorithms and seeking best practices for their applications. Their curiosity and determination inspired me to delve deeper into explaining these complex concepts. I wanted to bridge the gap between theory and practice, providing not just the tools but also the understanding necessary to use them effectively.This book is the culmination of that effort. In its pages, I've tried to address the root causes of the challenges faced by those new to the field. Through detailed explanations, underlying theory, and vivid visualizations, I aim to make advanced topics in machine learning accessible to all. My hope is that this resource will serve as a valuable guide for anyone looking to deepen their understanding of data science.I dedicate this book to all my students—for their relentless pursuit of knowledge, their thoughtful questions, and the mutual respect we've cultivated. Their success stories are the true measure of my work's impact, and their journeys continue to inspire me every day. This is not just a book; it's a shared voyage into the ever-evolving world of data science and machine learning. Together, we'll navigate the complexities and unlock the potential that data holds. "
 ],
 [
 "before-we-start.html",
 "Before we start",
 "Before we start Are you a budding machine learning engineer using Python, wondering what steps to take after mastering the basics? Perhaps you're a researcher exploring traditional machine learning algorithms for your thesis, a data scientist in your first role seeking to elevate model performance, or a leader transitioning into data science who wants to confidently engage with technical teams. This book is crafted precisely for you. It serves as a second course, aiming to answer the pressing question: What do I do now to improve my machine learning models? Our goal is to make you self-sufficient in the fine art of developing highly accurate machine learning models that are production-ready. We'll guide you through the different stages of a machine learning project—from advanced feature engineering to comprehensive model explanation. By learning these fundamental building blocks, you'll become a better-educated machine learning engineer, a more insightful researcher, a capable consultant, and a confident leader in the field of artificial intelligence and machine learning. This book is neither heavy on mathematical notation nor overloaded with code. Instead, we focus on explaining theoretical nuances, often aided by plots and analysis results, to provide a hands-on feel of projects from inception to completion. Supplemental code is available on the accompanying GitHub page, allowing you to delve deeper into how each analysis was conducted. We've organized the content to mirror the practical execution of a machine learning project, starting from feature engineering and moving towards model explanation. In Chapter 11, we delve into methods beyond traditional feature engineering and selection, introducing denoising techniques for signal processing data. While we cover multiple approaches to improving model performance, it's important to acknowledge that there's no silver bullet in machine learning. Even after employing techniques like feature engineering, feature extraction, feature selection, and ensembling, you might not achieve the desired model performance. Factors like data inadequacy and quality issues—topics beyond the scope of this book—can lead to project challenges. To illustrate these points, we'll examine four example datasets: two showcasing machine learning successes and two highlighting failure scenarios. Deep learning models often eliminate the need for feature engineering, especially with unstructured data like images, text, audio, and video. However, for structured tabular data, mastering feature engineering and knowing how to enrich your dataset by creating higher-level features from original ones is crucial. Understanding which features to retain can decisively impact your project's outcome. This book makes a sincere attempt to educate you on these different steps involved in training a machine learning model within the realms of feature engineering and feature selection. We assume you're already familiar with fundamental concepts like regression and classification, as well as common machine learning techniques such as linear regression, random forests, and logistic regression. You should also be comfortable with different cost functions like Root Mean Square Error (RMSE), F1 score, precision, and recall. This book is tailored for those who have tried their hand at developing models and are seeking guidance to elevate model performance to higher levels. Although the concepts discussed are applicable across various programming languages, this book is specifically written for Python users. To support this, several algorithms mentioned have been developed from scratch and open-sourced as four separate Python libraries by the author. We expect you to have a basic understanding of libraries like Scikit-learn for machine learning, Pandas and NumPy for data manipulation, and Matplotlib and Seaborn for data visualization. For experienced consultants, we introduce methods and techniques that are less commonly heard of or used, including those that were not previously available in Python. For leaders in data science, this book will help you gain perspective on how to advocate for your models by utilizing model explainability techniques. It's important to note that this book focuses on model development after data cleaning has been performed and you have a clean dataset. We do not cover how to clean data, handle outliers, or deal with missing data. Numerous valuable resources already exist on these topics, and they are outside the scope of this book. "
 ],
 [
 "section-1.html",
 "Section I: Introduction",
 "Section I: Introduction "
 ],
 [
 "introduction.html",
 "Chapter 1: Introduction",
 " Chapter 1: Introduction Quite often machine learning projects are shelved even when labeled data is available. This happens either because of less than desired predictive performance of the model. Or difficulty in being able to explain model prediction to non-statistician decision-makers. Building a machine learning model for real-world problems is way more difficult than developing a model for toy datasets. Real-world machine learning problems deal with inadequate features and messy, unclean data. Beyond data cleaning, we also need to do feature engineering, feature extraction, and feature reduction. Finding the combination of features that gives the best predictive performance is important for the success of the project. It is equally important to explain to non-statistician business owners how the model works. Why it predicts certain values, given certain input values? Doing so will ensure that the machine learning project is successful and the model is finally deployed and used. The model is of value to the business. The scope of the book is limited to tabular data. Toward the end of the book, we will briefly cover natural language processing from the perspective of traditional machine learning. An additional chapter on signal processing is provided for readers whose work overlaps signal processing and machine learning. We will briefly discuss ensemble learning and how we can do feature selection for ensemble models to reduce the complexity and computational power needed while deploying such an ensemble model. We will explain the theory behind each technique and for most of the methods discussed, we will end with a worked python example. By the time you have completed reading the book, you will take each machine-learning project as a combinatorial problem. You will initiate your projects to identify the combination of feature engineering and feature selection to achieve the best possible model performance. You will also be able to explain your model predictions, as to why it predicts certain values when it sees certain feature values. This book will equip you with all the necessary tools and methods to increase the likelihood of success of your machine learning projects. "
 ],
 [
 "terminology.html",
 "1.1: Terminology",
 " 1.1: Terminology Over the next section in this chapter and subsequent chapters, we will use a few terms frequently. Let's understand the meaning of these terms. 1.1.1  Dataset, Variable, and Observation If we are considering a CSV file that has relevant data for a machine learning project, we will refer to it as a dataset. Each column has data values and is of value for the project. We will refer to it as a  variable . A variable could be considered a dependent or target or outcome variable if it is the central point of focus of the project. The variables that we want to use for modeling the behavior of the target variable could be referred to as features or independent variables. Each row in the dataset will be referred to as an observation or record. 1.1.2    Feature Engineering Feature engineering or feature construction is the process of creating new features in the data set using domain knowledge, as well as creating higher order features from original features. A bank employee responsible for the upkeep of minimum cash reserve in a bank branch might be aware more customers visit branches for cash withdrawals just a few days before holidays, than the number of customers who visit during other days. Similarly, fewer customers might come to the branch for cash withdrawals, on the next day of the festival. This information can be used for creating 2 separate binary 1|0 indicator features to represent days before and after the festivals. This is an example of feature engineering using domain knowledge. In addition to domain knowledge, we can also use data-driven approaches for feature construction, such as exploratory data analysis (EDA). For example, during EDA if we observe data anomalies, we can probe this with subject matter experts and if it indeed turns out to be a valid pattern in the data, and is of use for the machine learning project, we can create features that represent the pattern in raw data. We can also create new features from existing features using transformations of the original features. For example, instead of taking original values, we can take the square of the original value to increase the magnitude. If body weight recorded for an adult is 71.9, 72, and 69.9, etc., we can instead take 5169.61, 5184, and 4886.01, which are squares of the original values. By performing a square, the difference in weight is more noticeable than the original values. 1.1.3    Feature Extraction Feature extraction is the process of finding alternative representations for the original features created during feature engineering. It converts features into a lower dimension. This makes the structure of the data clearer for the model to learn. For example, if there are 500 features, a technique such as principal component analysis (PCA) can convert these features into a lesser number of principal components, say 20. It might be easier and faster for the model to learn from fewer features that have maximum information. 1.1.4    Feature Selection Feature selection deals with choosing a subset of features from a list of given features. This helps identify features that are rich in useful information for the model, ultimately resulting in a less complex model of high predictive power. For example, if we have 5000 features, by performing feature selection we can identify a very small number of features that gives the highest performance. By having a small set of features, it also becomes easier to explain model prediction to laymen. 1.1.5    Cost Function  Cost function ,  model metric ,  model performance , and  predictive performance  are used interchangeably as synonyms in this book. For a regression model, example cost functions are root mean square error (RMSE), mean absolute error (MAE), etc. For a classification model, example cost functions could be F1 score, precision, and recall."
 ],
 [
 "training-a-model.html",
 "1.2: Process of Training a Machine Learning Model",
 " 1.2: Process of Training a Machine Learning Model There are 2 steps for training a machine learning model for structured data. Feature engineering is the bare minimum needed for developing a model using a dataset. Feature selection is performed on a need basis if the features created through feature engineering can t help us develop a model of desirable predictive power. After these 2 steps are performed, we can also perform hyperparameter tuning and ensemble learning. Although hyperparameter tuning and ensemble learning are not the book's focus, hyperparameter tuning and ensemble learning are briefly discussed in sections 11.1 and 11.2 in chapter 11."
 ],
 [
 "preventing-overfitting.html",
 "1.3: Preventing Overfitting",
 " 1.3: Preventing Overfitting A dataset can be divided into a training dataset, and external test data in an 80:20, 70:30, or even 60:40 ratio, depending on the availability of data. Then a further split for validation data is created from the remaining training data. The remaining training dataset can be divided further into train and development test data in the same ratio. We then train our model in iterations after performing feature engineering and feature selection. At each iteration, we train our model, and we test its performance on the development test data and validation data. Once we get a model of acceptable performance, we test it on external test data to ensure it generalizes well on unseen data. Bias-variance tradeoff tells us that by adding additional features, overfitting can happen. Overfitting can also happen by doing feature selection. If the model has learned our data in a way that gives the best predictive power for the dependent variable, it doesn't guarantee that it will generalize well on unseen data. What it only guarantees is that it can perform well on the development test data and validation data for validating our model. One way to overcome this challenge is to test your model on multiple development test data to understand how well it can generalize. This can be performed by cross-validation. Generally, we perform 5-fold cross-validation. We obtain the training dataset after separating external test data and validation data. The remaining training data is divided into 5 equal parts, and 4 parts are used for training. The remaining 1 part is for the development test. We also use validation data for measuring model performance. We repeat this 5 times and average the model metric across 5 samples for the development test data and validation data. Doing this reduces the likelihood of overfitting. In addition to cross-validation, we will test the model on external test data to ensure that indeed the cross-validation score is reliable. If the average of the model metric of 5 cross-validations is very different from the model metric of external test data, we will need to do further probing in the dataset to check if the data distribution is very different in 5-fold cross-validation and validation data Vs external test data. The object of the probe should be to ensure that data distribution is close to the real-world values in all 3 datasets. i.e. 1) training, 2) development test data in all cross-validation samples, 3) validation data, as well as in 4) external test data. If any anomaly is identified, it should be subject to further investigation. Using 5-fold cross-validation is subject to the availability of data. Although used as a universal standard, we should use 5-fold only when we have sufficient data. In the absence of volume, we can increase cross-validation from 5-fold to 10-fold, to get a more robust averaged model metric. 'More' and 'less' are subjective when it comes to datasets. For example, for the training of the Twitter sentiment classifier, 8000 tweets can be considered small, if we compare it against billions of tweets present in the Twitter database. If we still have to use such a small number of tweets dataset for modeling, it will be wise to use 10-fold cross-validation. However, while modeling the average lifespan of endangered species, 8000 observations can be considered voluminous data and we can limit cross-validation to 5-fold. If the data has distinct strata, we should ensure that train and test samples in cross-validation, as well as external hold-out samples, should all be representative of all the strata. We should proceed with stratified k-fold cross-validation in such cases. For example, if we are developing a model for predicting the likelihood of occurrence of a specific disease amongst people living in the US, given all the body vitals and characteristics such as blood pressure, sugar level, heartbeat rate, sodium level, cholesterol, daily minutes of exercise, body weight, height, etc. Such a dataset should be collected from all the states. Overall health and well-being might have less impact on a person based on which state they belong to. Hence, we cannot use the state as a feature to train our model. However, we should ensure that in the training and test samples across all cross-validations, each state should have an equal percentage of observations. This will ensure that model will generalize well to real-world circumstances. While doing train vs test split and even for external test data, we should give due regard to the unit of data. Most often one single observation is a unit of data. We can split such a dataset into cross-validation samples. However, when a data observation belongs to a distinguishable group of similar observations, as a single unit, we will consider it as a unit of data. For example, if we are evaluating the effectiveness of new medicine in regulating the blood pressure of individual patients, we will record the dosage amount and resultant blood pressure for multiple days. In this case, data recorded for all days for a specific patient will be a unit of data. While splitting data into the training set, external test data, and development test data, we should ensure that a patient's data is present entirely either in training, external test data, validation data, or development test data. In no case, a patient's data should be present in more than one group from training, external test data, or development test data. If a patient's data is present in both training and test data simultaneously, then the results could be biased, as it will lead to over-optimistic results for the same patient, as the model is already familiar with similar values for the patient in training data. At an aggregate level, it can give us overoptimistic model performance. Hence, we should identify units of data and should ensure that data from a single unit is not present in training, test, and/or external test data simultaneously. If cross-validation is done right, feature engineering and feature selection can help us identify a smaller set of features that give the best performance for predicting the target variable. "
 ],
 [
 "code-conventions.html",
 "1.4: Code Conventions",
 " 1.4: Code Conventions Most of the codes are provided in the accompanying GitHub repository [1]. We will use code sparingly in the book and instead focus on understanding the key concepts. Wherever we have used code, comments for the code will be presented in the below format # We will add 1+1 Code will be in the below format. print(\"1+1 is: \"+str(1+1)) "
 ],
 [
 "datasets-used.html",
 "1.5: Datasets Used",
 " 1.5: Datasets Used We will use 4 separate datasets throughout this book for regression and classification problems. We will benchmark model performance and compare the performance of different techniques. These datasets are explained in the first four datasets from sections 1.5.1 to 1.5.3. We will also use 2 datasets for signal processing in chapter 11. These datasets are explained in sections 1.5.4 and 1.5.5. 1.5.1    Hotel Booking Demand Datasets This dataset [2] has demand data for 2 hotels. Hotel H1 is a resort hotel that attracts customers who will like to stay at the hotel for recreation purposes. Hotel H2 is a city hotel, that people visit for business purposes. We will use data from hotel H1 for developing classification models to predict the likelihood of cancellation for reservations. The objective of the classification model is to minimize losses from cancellations and increase profitability. Many customers cancel their reservations. This could mean a loss of revenue. If we know in advance which reservation is going to be canceled, we can try to resell the room, even though it is not canceled at the moment. However, it can lead to a situation when both guests arrive at the hotel and demand the room they have reserved. To avoid this situation, our modeling objective is to build a model which can predict cancellation with a high degree of accuracy. In other words, 'precision' will be our cost function. As a result, we might have a model which has a low recall, but high precision. We are fine with such a model, as long as it can predict cancellations with a low margin of error. Imagine a hypothetical model that has 95 precision and 30 recall. It simply means, for all predictions it makes for cancellations, 95 times out of 100 could actually be a cancellation. Although the precision is not the ideal 100, 95 precision is a good score for a statistician. A low recall of 30 means, out of all cancellations that happened, the model could only identify 30 percent of such cancellations. For a statistician or machine learning engineer, recall value of 30 is a less than ideal situation. However, for business owners, this is an acceptable situation. For a hotel revenue manager, recall value of 30 means that the hotel will be able to reduce their cancellation losses by 30 percent. This is still better than a situation where the hotel cannot save anything without the model. If not all, in many cases businesses desire a workable model. A perfect model which meets all criteria of theoretical statistics and theoretical machine learning is not always desired by real world businesses. Data for hotel H2 will be used for developing a regression model to predict the total occupancy for rooms for future check-in dates. This dataset was not in the format where it could have been used for the regression model. The author has used his domain knowledge in the hotel industry to perform preprocessing and data cleaning to bring it into a usable format. Hotels have different types of customers. Two major groups are 'transient' and 'group'. Transient customers often seek short hotel stays, such as people who travel to different cities for business purposes, or other customers who want short-term stays. We will like to develop a model to predict total transient bookings at the check-in date level, at each booking window between 0 to 100 days before the date of check-in for the city hotel H2. Booking day is the day when customers book the room for the check-in date in advance. For example, if you want to visit Hawaii on Christmas of 2022 and you are reserving the room on the 1st of October, 2022, then October 1st, 2022 will be the booking date and December 25th, 2022, the day of Christmas will be the check-in date. The number of days between these two dates is called lead time. Sometimes customers book rooms, but they do not show up. We will remove these customers from our modeling problem, as these are very negligible. ADR is the average daily rate, for any check-in date it represents total revenue from bookings divided by total rooms sold. We will remove records for ADR which seem outliers, as we cannot get any explanation from the original authors about the nature of outliers. We will use ADR between the bottom 5th percentile and the top 99.99 percentile. There were transactions for which no value was specified for the number of adults and children as guests. These will also be removed. Some customers buy rooms for continuous stay. If a customer is reserving on the 1st of January for staying at the H2 hotel on the 7th and 8th of January, the lead time for the 7th of January will be 6 days and for the 8th of January will be 7 days. We assume that one room can accommodate 2 adults and 2 kids in a hotel. If a transaction has 2 adults and 2 kids, it will be considered as 1 room. If a transaction has 1 adult and 2 kids, 2 adults, and 1 kid, or 1 adult and 1 kid, it will be considered as 1 room. If there are more than 2 adults, the number of rooms will be calculated as the number of adults divided by 2. If the result is a decimal, it will be rounded to a higher number. For example, if there are 3 adults, 3 divided by 2 is 1.5. We will round 1.5 to a higher ceiling of 2. Hence concluding that 3 adults can stay in 2 rooms. Similarly, children are also considered for calculating the number of rooms. 1.5.2    Car Sales This is sourced from Kaggle [3] and has information about the attributes of used cars and its price in India. It will be used for developing regression models. We will use the 'Car details v3.csv' file from version 3 of the dataset. The original file has details of 8128 cars. We have removed certain records which had inadequate data or missing data. These are mileage features for values of 0.0kmpl or left blank. Engine, torque, or seats are left blank. max_power as 0, 'bhp' or blank. km_driven as 1. After removing these observations, 7905 observations remain. Mileage, engine, and max_power were converted to numeric features after removing string suffixes. 1.5.3    Coupon Recommendation In-vehicle coupon recommendation Data Set [4] studies the behavior of individuals and whether they will accept or not accept coupons. This dataset was generated through a survey. This survey describes different driving scenarios and asks the person if they will accept the coupon. 1.5.4    Raman Spectroscopy of Skimmed Milk Samples This dataset [5] has a Matrix of quantitative whole spectrum analysis of 45 spectra on 21451 on skimmed milk samples. We will use this for discussing Raman spectra in chapter 11. 1.5.5    Beaver Body Temperatures This dataset has body temperature of 2 beavers [6], measured every 10 minutes by telemetry. This has data for less than a day for each of the two beavers. We will discuss this in chapter 11 for filtering method. "
 ],
 [
 "chapter1-references.html",
 "1.6: References",
 " 1.6: References [1] https://github.com/statguyuser/feature_engineering_and_selection_for_explanable_models [2] Nuno Antonio, Ana de Almeida, Luis Nunes, Hotel booking demand datasets, Data in Brief, Volume 22, 2019, Pages 41-49, ISSN 2352-3409, https://doi.org/10.1016/j.dib.2018.11.126. [3] https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho?select=Car+details+v3.csv [4] Wang, Tong, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. 'A bayesian framework for learning rule sets for interpretable classification.' The Journal of Machine Learning Research 18, no. 1 (2017): 2357-2393. [5] Kristian Hovde Liland, Bj rn-Helge Mevik, Elling-Olav Rukke, Trygve Alm y, Morten Skaugen and Tomas Isaksson (2009) Quantitative whole spectrum analysis with MALDI-TOF MS, Part I: Measurement optimisation. Chemometrics and Intelligent Laboratory Systems, 96(2), 210 218. [6] P. S. Reynolds (1994) Time-series analyses of beaver body temperatures. Chapter 11 of Lange, N., Ryan, L., Billard, L., Brillinger, D., Conquest, L. and Greenhouse, J. eds (1994) Case Studies in Biometry. New York: John Wiley and Sons."
 ],
 [
 "restricted.html",
 "Section II: Feature Engineering",
 "Section II: Feature Engineering"
 ],
 [
 "baseline-removal-python.html",
 "Chapter 11: Things That Can Give Additional Improvement",
 " Chapter 11: Things That Can Give Additional Improvement     11.1: Introduction There are factors beyond feature engineering and feature selection that can help in improving the predictive power of the model. These are data cleaning, domain-specific feature engineering, ensemble learning, and hyperparameter optimization. We will briefly discuss hyperparameter optimization and ensemble learning in this chapter."
 ],
 [
 "hyperparameter-tuning.html",
 "11.2: Hyperparameter Tuning",
 "11.2: Hyperparameter Tuning Many machine learning algorithms have hyperparameters that help the algorithm learn effectively from the data. Increasing or decreasing the value of these hyperparameters can alter the performance of the model for better or worse. For example, random forest and many other tree algorithms have a parameter for defining the number of trees that will be used for the model. By increasing or decreasing the number of trees, model performance might change. If the number of trees is low, say 1, and we increase it to 5, the performance of the model might improve. If we further increase from 5 to 20, the model might perform even better. However, if we keep increasing the number of trees, it might not always lead to a better-performing model. After a certain number of trees, the model performance may not increase, or worse, performance might decrease. Hence it is very important to identify the number of trees in this case. Some machine learning techniques have multiple hyperparameters. Not all hyperparameters are equally important for improving model performance. Depending on the computing power and time available, we should try hyperparameter tuning. There are many techniques available for hyperparameter tuning. The most basic method of hyperparameter optimization is called manual search. In this method, we try all possible combinations for feature selection. A for loop can help us perform all possible combinations to fit the model with different values of hyperparameter and identify the combination at which the model gives the best performance. There is another method for hyperparameter search known as grid-search. This method is an improvised version of manual search hyperparameter optimization. The only advantage this method has is that it saves us from writing multiple nested loops. Both manual and grid search methods are computationally expensive. Randomized Search method on the other hand trains the model on random hyperparameter combinations. It saves us from trying all combinations of hyperparameter values. One disadvantage of this method is that it may not identify the optimal values of hyperparameters. Bayes Grid Search uses Bayesian optimization to identify optimal values of hyperparameters. It can be done with the help of the python library skopt. There is another python library optuna, that uses the define-by-run principle to enable the user dynamically construct the search space. It allows the user ways that have never been possible with previous hyperparameter tuning frameworks. It does so by combining efficient searching and pruning algorithms, and thereby greatly improves the cost-effectiveness of optimization. "
 ],
 [
 "ensemble-learning.html",
 "11.3: Ensemble Learning",
 "11.3: Ensemble Learning For the case of room booking prediction for hotels, let's imagine that linear regression predicts with a high degree of accuracy for December month and random forest for the first 6 months of the year. Xgboost on the other hand outperforms for the remaining months of the year. If we can leverage the individual strength of different models, we can come up with a highly accurate model for predicting hotel booking. Ensemble learning helps us perform just that by leveraging the strength of different models to provide a highly reliable prediction. The simplest form of ensembling is called averaging, wherein we take the prediction from multiple models and take a mathematical average of prediction from all the models to arrive at the final prediction. If some modeling technique performs better than others, we can give weights to each model, which signifies the degree to which the model is accurate. This will allow us to perform weighted average ensembling. Ensembling by conditional averaging is another approach in which we take a simple average from 2 models based on some conditions laid upon output values. For example, if there are 2 models and model 1 predicts accurately values under 50 and model 2 predicts values above 50. We can simply add an if-else condition for prediction on model 1. Bagging is a useful form of ensembling for high-variance techniques. The high variance techniques are prone to overfitting, such as decision trees. We can create multiple models of the same techniques, simply by changing random seed, parameter tuning, changing the number of features and records in the dataset, etc. This can be impactful if we have a larger number of models. We can also use BaggingClassifier and BaggingRegressor in Sklearn to do the same task. In contrast, for boosting-based ensemble learning, newer models are added sequentially depending on how well the previous model performed. We can also give weight for performing boosting. Another form of ensemble learning is called stacking. For stacking, training data is divided into two parts. One part is used for training models using a different diverse set of techniques. These individual models then predict values for the second part of the dataset. For the second part of the dataset, predictions obtained from individual models are used as features. By using the dependent variable in the second part of the training dataset, a final ensemble model is trained. For stacking to be successful, we should try a different diverse set of techniques, such as linear, non-linear, etc. "
 ],
 [
 "signal-processing-machine-learning.html",
 "11.4: Signal Processing",
 " 11.4: Signal Processing It is the domain that deals with analyzing, modifying, and synthesizing signals. A signal can be audio, video, radar measurement, etc. It converts and transforms data to enable us to see things that are not possible via direct observation. The most common applications of signal processing are audio and video compression, speech recognition, improving audio quality in phone calls, oil exploration, etc. Signal processing can help us extract important parts of the signal, which can then be used as features to train the model. If the features are derived from signals, it can help if we clean the features using signal processing techniques methods. We will discuss two such methods which are useful for machine learning applications. Filtering of signals, and baseline removal for Raman spectra. 11.4.1  Filtering In signal processing, filtering denotes removing unwanted frequencies and frequency bands from the signal. It helps in increasing the precision of the data without distorting the signal. It is performed through a process known as convolution. It fits subsets of adjacent data with low-degree polynomials using linear least squares. It has wide use in radio, music synthesis, image processing, etc. Savitzky-Golay filter is one of the commonly used methods for removing noise from data. Let s discuss some practical machine-learning applications that use filtering. Savitzky-Golay filter has been used [1] in demand forecasting for eliminating outliers and noises in the non-stationary time series. This helps time series models to learn better from the filtered data and forecast more accurately. It can also be used with deep learning. For example, it has been used [2] with one-dimensional CNN layers to identify abnormal EEG signals, without using any explicit feature extraction technique. We will analyze the beaver body temperature discussed in chapter 1. There are 2 beavers, and we will only analyze the body temperature of beaver 1 for the sake of simplicity. Let's look at beaver1's body temperature with and without filtering in figure 11.4.1. We can see that filtered temperatures have less volatility and are more stable. It retains the information about patterns in temperature, and at the same time filters and suppresses possible noise and extreme values. We used the polynomial order of 5 and a window length of 11 for filtering. If needed, we can increase or decrease the values, based on observed patterns in the data to help the algorithm filter noise more accurately. Figure 11.4.1: Beaver1 body temperature with and without Savitzky-Golay filtering 11.4.2  Baseline Removal Raman spectra are widely used in different scientific fields that focus on studying macromolecules. It allows both chemical and physical structural analysis of materials, using a small sample, without damaging the samples. This is used by law enforcement agencies for identifying contraband items, without physically inspecting them. It can also be used for detecting diseases [3], without any need for further medical diagnostics. Despite its usefulness, Raman spectra has one issue that needs to be taken care of before using. It carries a background, otherwise known as the baseline. Unless treated and removed, the baseline can cause negative effects in the qualitative and quantitative analysis of spectra. Hence, Raman spectra are fitted and corrected to mitigate this negative influence before being used. There are many methods of correcting the baseline. We will discuss 3 methods with the help of the companion python library BaselineRemoval and see how the three algorithms can help remove background from the spectra. Modified multi-polynomial fit, also known as ModPoly uses thresholding, to iteratively fit a polynomial baseline to data. Its limitation is that it is prone to variability in data which has a low signal-to-noise ratio. It can smoothen the spectrum by automatically eliminating Raman peaks and leaving behind baseline fluorescence, which can finally be subtracted from the raw spectrum. It uses least-square polynomial fitting functions. Data points are generated from this curve. Data points with higher values than the respective input values are assigned to the original intensity. This exercise is repeated for several iterations, between 25 to 200. The number of repetitions depends on factors such as the relative amount of fluorescence to Raman. There are some major limitations for ModPoly, as it is dependent on the spectral fitting range and the polynomial order specified. It might not be an ideal solution for high-noise situations, as noise is not appropriately dealt with in ModPoly. ModPoly tends to introduce artificial peaks in the data in places where the original spectrum was free of such peaks. Also, existing large peaks in the data tend to contribute more to the polynomial fitting, which can in turn bias in the results. Improved ModPoly, also known as IModPoly is an improvement on the ModPoly algorithm and is meant for noisy data. Identifying and removing major peaks is limited to the first iteration only. This prevents unnecessary data rejection. For each iteration of polynomial fitting, lower values of the wave number are selected and concatenated. This is used for constructing a modified spectrum. This in turn is then fitted again. Despite the improved version of the algorithm, IModPoly, just like its predecessor, requires user intervention and prior information, such as detected peaks. A new method was proposed by Zhang [4], which doesn t require any user intervention and prior information, such as detected peaks. It is named adaptive iteratively reweighted penalized least squares. It is a fast and flexible method that performs adaptive iteratively reweighted penalized least squares. This helps in approximating complex baselines. In each iteration, weights are obtained adaptively by using SSE between a previously fitted baseline and original signals. It uses a penalty approach to control the smoothness of the fitted baseline. It does so by using the sum squared derivatives of the fitted baseline. Lambda is a parameter controlled by the user. Larger the lambda, the smoother the fitted vector. Let s now look at data distribution for original spectra and baseline corrected spectra for the skimmed milk samples discussed in chapter 1. Figure 11.4.2.1: Original Vs. ModPoly corrected spectra of skimmed milk samples We can see in figure 11.4.2.1 that artificial peaks are introduced by ModPoly for observations at 8000 till 20000. In this region, ModPoly is higher than the original spectrum. Figure 11.4.2.2: Original Vs. IModPoly corrected spectra of skimmed milk samples As seen in figure 11.4.2.2, ImodPoly performs better than ModPoly. It removed noise from the spectra, as seen in the plot, between 0 to 8000, and further after 19000. Also, unlike ModPoly, it didn t add an artificial peak. Figure 11.4.2.3: Original Vs. airPLS corrected spectra of skimmed milk samples We can see in figure 11.4.2.3 for the airPLS method that it removed noise from spectra better than previous algorithms. Especially, for observations between 0 and 2500. As the denoised spectra in this section resemble closely with the rest of the spectra."
 ],
 [
 "conclusion.html",
 "11.5: Conclusion",
 "11.5: Conclusion We should honor all the fine details in a dataset. If a dataset has geospatial properties, we should account these properties as features in the model. If the features we obtained as signals from devices, we should perform suitable signal processing techniques before developing a model. Doing so will ensure we have information rich features, less outliers and better performing models."
 ],
 [
 "references.html",
 "11.6: References",
 " 11.6: References [1] Jing Bi, Haitao Yuan, LiBo Zhang, Jia Zhang, SGW-SCN: An integrated machine learning approach for workload forecasting in geo-distributed cloud data centers⁎⁎This paper belongs to the special issue special issue name edited by Prof. W. Pedrycz. , Information Sciences, Volume 481, 2019, Pages 57-68, ISSN 0020-0255, https://doi.org/10.1016/j.ins.2018.12.027. [2] U. Shukla, G. J. Saxena, M. Kumar, A. S. Bafila, A. Pundir and S. Singh, \"An Improved Decision Support System for Identification of Abnormal EEG Signals Using a 1D Convolutional Neural Network and Savitzky-Golay Filtering,\" in IEEE Access, vol. 9, pp. 163492-163503, 2021, doi: 10.1109/ACCESS.2021.3133326. [3] Gonz lez-Sol s, J.L., Mart nez-Espinosa, J.C., Torres-Gonz lez, L.A. et al. Cervical cancer detection based on serum sample Raman spectroscopy. Lasers Med Sci 29, 979 985 (2014). https://doi.org/10.1007/s10103-013-1447-6 [4] Zhang ZM, Chen S, Liang YZ. Baseline correction using adaptive iteratively reweighted penalized least squares. Analyst. 2010 May;135(5):1138-46. doi: 10.1039/b922045c. Epub 2010 Feb 19. PMID: 20419267. "
 ]
]