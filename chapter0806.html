<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8.6 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
  </title>
  
  
  <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
  <script src="libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
   

  <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
  <script src="javascript/cookieconsent.min.js"></script>
  <script>
    window.addEventListener("load", function () {
      window.cookieconsent.initialise({
        "palette": {
          "popup": {
            "background": "#000"
          },
          "button": {
            "background": "#f1d600"
          }
        },
        "position": "bottom-right",
        "content": {
          "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
        }
      })
    });
  </script>

  <style>
    #cta-button-desktop:hover,
    #cta-button-device:hover {
      background-color: #ffc266;
      border-color: #ffc266;
      box-shadow: none;
    }

    #cta-button-desktop,
    #cta-button-device {
      color: white;
      background-color: #ffa31a;
      text-shadow: 1px 1px 0 #444;
      text-decoration: none;
      border: 2px solid #ffa31a;
      border-radius: 10px;
      position: fixed;
      padding: 5px 10px;
      z-index: 10;
    }

    #cta-button-device {
      box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
      display: none;
      right: 20px;
      bottom: 20px;
      font-size: 20px;
    }

    #cta-button-desktop {
      box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
      display: display;
      padding: 8px 16px;
      right: 40px;
      bottom: 40px;
      font-size: 25px;
    }

    @media (max-width : 450px) {
      #cta-button-device {
        display: block;
      }

      #cta-button-desktop {
        display: none;
      }
    }
  </style>






  <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">

        <ul class="summary">
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Summary</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                class="fa fa-check"></i>Foreward</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
              href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
          <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                class="fa fa-check"></i>Before we start</a></li>
          <li class="chapter" data-level="1"><a href="section01.html"><i class="fa fa-check"></i>Section I:
              Introduction</a>

          <li class="chapter" data-level="1"><a href="chapter01.html"><i class="fa fa-check"></i>1: Introduction</a>
          </li>

          <ul>
            <li class="chapter" data-level="2"><a href="chapter0101.html"><i class="fa fa-check"></i>1.1:
                Terminology</a></li>
            <li class="chapter" data-level="2"><a href="chapter0102.html"><i class="fa fa-check"></i>1.2: Process of
                Training a Machine Learning Model</a></li>
            <li class="chapter" data-level="2"><a href="chapter0103.html"><i class="fa fa-check"></i>1.3: Preventing
                Overfitting</a></li>
            <li class="chapter" data-level="2"><a href="chapter0104.html"><i class="fa fa-check"></i>1.4: Code
                Conventions</a></li>
            <li class="chapter" data-level="2"><a href="chapter0105.html"><i class="fa fa-check"></i>1.5: Datasets
                Used</a></li>
            <li class="chapter" data-level="2"><a href="chapter0106.html"><i class="fa fa-check"></i>1.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
              Feature Engineering</a>
          <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain Specific
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                Domain-Specific Feature Engineering </a></li>
            <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                References</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA Feature
              Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2: Car Sales
              </a></li>
            <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3: Coupon
                Recommendation</a></li>
            <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher Order
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                Engineering Categorical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                Engineering Ordinal Features </a></li>
            <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                Engineering Numerical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                Conclusion</a></li>
          </ul>
 <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5: Interaction
                Effect Feature Engineering</a>
            </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                Interaction Plot</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2: SHAP</a>
            </li>
            <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                Conclusion</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
              Feature Selection</a>
          <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6: Fundamentals of
              Feature Selection</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2: Different
                Feature Selection Methods</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3: Filter
                Method</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4: Wrapper
                Method</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
              Selection Concerning Modeling Techniques</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                Lasso, Ridge, and ElasticNet</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2: Feature
                Importance of Tree Models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3: Boruta</a>
            </li>
            <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4: Using
                Tree-Based Feature Importance for Linear Model</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5: Using
                Linear Model Feature Importance for Tree Models</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6: Linear
                Regression</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7: SVM</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8: PCA</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
              Selection Using Metaheuristic Algorithms</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1: Exhaustive
                Feature Selection</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2: Genetic
                Algorithm</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3: Simulated
                Annealing</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4: Ant Colony
                Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5: Particle
                Swarm Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
              Model Explanation</a>
          <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining Model
              and Model Predictions to Layman</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                Explainable models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                Explanation Techniques</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
              Special Chapters</a>
          <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
              Engineering & Selection for Text Classification</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2: Feature
                Construction</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3: Feature
                Selection</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4: Feature
                Extraction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5: Feature
                Reduction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                References</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things That Can
              Give Additional Improvement</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                Hyperparameter Tuning</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3: Ensemble
                Learning</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4: Signal
                Processing</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                References</a></li>
          </ul>
          </li>
        </ul>

      </nav>
    </div>
    <div class="book-body">
      <div class="body-inner">
        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                <h1><span class="header-section-number">8.6:</span> Putting
                  Everything Together</h1>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>For certain models, such as Xgboost
                    and linear models, we have used GPU for training the model. For Lightgbm, we
                    have used a 64GB RAM machine to compute faster.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>These algorithms provide the best
                    possible combination of features. These are not necessarily the best and ideal
                    combination of features. As the result, there might still be opportunity for
                    improvement. If we use the feature set generated by a metaheuristic algorithm,
                    and perform feature selection again, there might be chance for even better
                    performance. We will see this for hotel total room booking prediction modeling,
                    where we perform genetic algorithm multiple times. In each new iteration of
                    genetic algorithm, we use output set of features from previous iteration of
                    genetic algorithm. In each iteration we try to get better performance than the
                    previous iteration.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%'><a name="_heading=h.meukdy"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                  color:windowtext'>8.6.1<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp; </span>Hotel Total
                      Room Booking<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We used all 4 metaheuristic
                    algorithms with the 3 models. Amongst all, Lightgbm and genetic algorithm
                    performed the best. While it performed best, there might still be a chance of
                    improving the feature combination. We used the list of features obtained from
                    the genetic algorithm as input features and performed the genetic algorithm a
                    few more times to refine the feature list even further. We performed this
                    exercise a total of 5 times and saw improvement for the 2nd, 3rd, and 4th
                    iterations. Beyond 4th iteration, there was no more improvement.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>For the genetic algorithm, we used a
                    population of 75 and executed 25 generations for the first iteration. For
                    subsequent iterations, the number of generations was reduced to 20. Output from
                    the iteration of the genetic algorithm was used as input for the next generation
                    of feature selection. At each iteration of the genetic algorithm, execution
                    time was limited to 1200 minutes.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>At 1st iteration, the number of
                    features from the genetic algorithm was 44. At the 2nd, 3rd, and 4th
                    iterations, the number of features reduced from 44 to 20, 10, and 9
                    respectively. Also, at the 4th iteration, RMSE was reduced to 8.63 for test and
                    validation datasets. RMSE for external test data also decreased to 8.79.
                    Results from each cross-validation are presented in figure 8.6.1.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="image31.png" o:spid="_x0000_i1067" type="#_x0000_t75" style='width:508pt;
                   height:271pt;visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image103.png"
                    o:title=""/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=677 height=361 src="images/image104.jpg" v:shapes="image31.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 8.6.1 performance of Lightgbm
                    tree model with genetic algorithm feature selection on cross-validation test,
                    validation, and external test data for hotel total room booking prediction<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>This is the best result found so far
                    for the hotel total rooms prediction dataset. RMSE of error is both the test
                    datasets are very similar, and there is little to no difference in results
                    across different cross-validations. Both of these factors suggest that the
                    model will generalize well on unseen data.<o:p></o:p></span></p>

                    <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                    0cm;line-height:150%'><a name="_heading=h.36ei31r"></a><b><span lang=EN
                    style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                    color:windowtext'>8.6.2<span style='mso-tab-count:1'>    </span>Hotel Booking
                    Cancellation<o:p></o:p></span></b></h4>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>Xgboost and simulated annealing
                    performed the best for the hotel booking cancellation dataset. It has 0.88 as
                    precision for cross-validation test and validation dataset. For the external
                    dataset, precision was noted as 0.93. This solution did have a downside with a
                    low recall at 0.41 for the external test data. For simulated annealing, we used
                    35 iterations and 75 perturbs for performing feature selection. Figure 8.6.2.1
                    explains the model performance for each cross-validation.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                     id="image32.png" o:spid="_x0000_i1066" type="#_x0000_t75" style='width:521pt;
                     height:274pt;visibility:visible;mso-wrap-style:square'>
                     <v:imagedata src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image105.png"
                      o:title=""/>
                    </v:shape><![endif]--><![if !vml]><img border=0 width=695 height=365
                    src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image106.jpg"
                    v:shapes="image32.png"><![endif]></span><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'><o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>Figure 8.6.2.1 performance of
                    Xgboost tree model with simulated annealing feature selection on
                    cross-validation test, validation, and external test data for hotel booking
                    cancellation<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>There is another solution which is
                    the combination of Xgboost and genetic algorithm, and it performed as second
                    best. It has 0.86 as precision for cross-validation test and validation
                    dataset. For the external dataset, precision was noted as 0.92. This solution
                    has a relatively better recall at 0.41 for the external test data. We used 20
                    generations for 75 chromosomes for the algorithm execution. Figure 8.6.2.2
                    explains the model performance for each cross-validation.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>Both the solutions, especially the
                    first solution is not perfect, but nearly useful. If the precision can be
                    reliably said to be above 0.9 and close to 0.95, the hotel can use the
                    predictions from the model for overselling the rooms. Even though the model has
                    a lower recall. To overcome this, we can try an ensemble of two or more models
                    to see its impact on precision and recall. Imagine a model that can predict
                    hotel booking cancellations with a high degree of reliability. Even if it can
                    identify only 40 percent of cancellations and cannot identify the rest of the
                    60 percent of the cancellations, it will still be useful for the hotel to
                    minimize loss occurring because of 40% of cancellations.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>Comparing the solutions obtained
                    through all 4 metaheuristics algorithms, these are the best result achieved for
                    this dataset, in comparison to other feature selection methods.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                     id="image34.png" o:spid="_x0000_i1065" type="#_x0000_t75" style='width:516pt;
                     height:272pt;visibility:visible;mso-wrap-style:square'>
                     <v:imagedata src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image107.png"
                      o:title=""/>
                    </v:shape><![endif]--><![if !vml]><img border=0 width=688 height=363
                    src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image108.jpg"
                    v:shapes="image34.png"><![endif]></span><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'><o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>Figure 8.6.2.2 performance of
                    Xgboost tree model with genetic algorithm feature selection on cross-validation
                    test, validation, and external test data for hotel booking cancellation<o:p></o:p></span></p>
                    
                    <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                    0cm;line-height:150%'><a name="_heading=h.1ljsd9k"></a><b><span lang=EN
                    style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                    color:windowtext'>8.6.3<span style='mso-tab-count:1'>    </span>Car Sales<o:p></o:p></span></b></h4>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>We tried different combinations of
                    metaheuristics algorithms and models. The best performance was achieved by the
                    combination of Lightgbm and simulated annealing. For cross-validation test and
                    validation data, RMSE was 197495, whereas for the external test data it is
                    224034. It is better than the results reported in chapter 7. For simulated
                    annealing, we used 35 iterations with 75 perturbations.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>Figure 8.6.3 shows the model
                    performance across different cross-validations. RMSE is not very consistent
                    across all cross-validations. RMSE is different across different test datasets.
                    However, the major issue in this dataset is that RMSE is still very high. For
                    this dataset, none of the feature engineering and feature selection helped us
                    achieve a workable model that can predict the price of used cars reliably. This
                    indicates that the dataset requires data cleaning and domain-specific feature
                    engineering. Domain knowledge in particular can help in organizing data that
                    can be easier for the model to learn, finding anomalies, and treating it to
                    ensure a dataset with the least amount of noise, and finally in creating
                    features that permeate from domain knowledge.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                     id="image39.png" o:spid="_x0000_i1064" type="#_x0000_t75" style='width:528.5pt;
                     height:272pt;visibility:visible;mso-wrap-style:square'>
                     <v:imagedata src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image109.png"
                      o:title=""/>
                    </v:shape><![endif]--><![if !vml]><img border=0 width=705 height=363
                    src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image110.jpg"
                    v:shapes="image39.png"><![endif]></span><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'><o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>Figure 8.6.3 performance of the
                    Lightgbm model with simulated annealing feature selection on cross-validation
                    test, validation, and external test data for used car price prediction.<o:p></o:p></span></p>
                    
                    <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                    0cm;line-height:150%'><a name="_heading=h.45jfvxd"></a><b><span lang=EN
                    style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                    color:windowtext'>8.6.4<span style='mso-tab-count:1'>    </span>Coupon
                    Recommendation<o:p></o:p></span></b></h4>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>For this dataset, Xgboost performed
                    the best with simulated annealing feature selection. It has 0.78 as precision
                    for cross-validation test and validation dataset. For the external dataset,
                    precision was noted as 0.79. This solution did have a downside with a low
                    recall at 0.59 for the external test data. For simulated annealing, we used 35
                    iterations and 75 perturbs for performing feature selection. Figure 8.6.4
                    explains the model performance for each cross-validation.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>This is the best performance that
                    could be achieved for the dataset, across all feature selection methods and
                    model techniques. However, these results are not good enough to be accepted as
                    a reliable model. If either the precision or recall could have been close to
                    0.9 or higher, we could have obtained a reliable model. Hence, although model
                    performance across different cross-validations is very similar, and there is a
                    very small difference between the two different test datasets, we will discard
                    this model.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'>In the absence of domain knowledge
                    for this dataset, we at this point halt any further effort to improve the model
                    for this dataset.<o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                     id="image38.png" o:spid="_x0000_i1063" type="#_x0000_t75" style='width:468.5pt;
                     height:247pt;visibility:visible;mso-wrap-style:square'>
                     <v:imagedata src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image111.png"
                      o:title=""/>
                    </v:shape><![endif]--><![if !vml]><img border=0 width=625 height=329
                    src="Feature%20Engineering%20&amp;%20Selection%20for%20Explainable%20Models%20-%20A%20Second%20Course%20for%20Data%20Scientists_html_files/image112.jpg"
                    v:shapes="image38.png"><![endif]></span><span lang=EN style='font-family:"Times New Roman",serif;
                    mso-fareast-font-family:"Times New Roman"'><o:p></o:p></span></p>
                    
                    <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                    8.0pt;margin-left:0cm;line-height:150%'><a name="_heading=h.2koq656"></a><span
                    lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                    "Times New Roman"'>Figure 8.6.4 performance of the Xgboost model with simulated
                    annealing feature selection on cross-validation test, validation, and external
                    test data for coupon recommendation prediction.<o:p></o:p></span></p>


              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="chapter0805.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="chapter0807.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
  

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>